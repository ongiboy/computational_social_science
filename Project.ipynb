{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Assignment B\n",
    "Link to git repository: https://github.com/ongiboy/computational_social_science\n",
    "\n",
    "Group members:\n",
    "* Christian Ong Hansen (s204109)\n",
    "* Kavus Latifi Yaghin (s214601)\n",
    "* Daniel Damkjær Ries (s214641)\n",
    "\n",
    "Group member's contribution:\n",
    "* Every task was made in collaboration by all members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is your dataset\n",
    "\n",
    "Our dataset originates from The Movie Database (TMDB) via its API. The data is collected from the \"Popular People\" tab on the webpage, and includes the top actors from the first 250 pages, with each page containing data for 20 actors. The popularity score attributed to actors by TMDB is a metric not publicly disclosed, but it's generally understood to consider factors like page views, favorites, watchlists, and recent activity.\n",
    "\n",
    "The data is structured into two main dataframes: one for actors and their attributes (name, ID, gender, age, birthplace, and filmography), and another for movies featuring these actors (rating, popularity, genres, release date, and abstract for text analysis). This setup forms the basis for our actor collaboration network and analysis, which is essential for answering our research question: Based on collaborations among the most popular actors, do distinct communities form, and if so, what characterizes the most succesful communities?\n",
    "\n",
    "* Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "The reason for us choosing The Movie Database (TMDB) as the data source for our project is because of our interest in exploring and analyzing the film industry by an actor collaboration network. TMDB has a broad coverage of the film industry and it has built a reputation for being a well-known data source within the field. TMDB is widely recognized for its movie-related data, containing details on actors, movies, genres, ratings, popularity etc.. The rate limit for the API (50 calls per second) was also an attractive factor. \n",
    "\n",
    "It was therefore believed that through TMDB's API, it would be possible to access up-to-date and reliable data on popular actors and their participation in movies, enabling us to explore actor interactions and movie trends effectively. In this way the TMDB datasource can be used to contribute valuable insights to the broader understanding of the film industry landscape.\n",
    "\n",
    "* What was your goal for the end user’s experience?\n",
    "\n",
    "The goal of this project for the end user's experience was to provide insights into the characteristics and dynamics of successful actor communities. By analyzing data from popular actors and their collaborations in a network, we sought to offer a user-friendly interface for exploring trends, identifying influential factors, and gaining a deeper understanding of what drives success in the film industry in an actor perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats\n",
    "* Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "Using the TMDB API it was possible to retrieve information about popular actors just by using an API-key and selecting the desired number of pages with 20 actors in each page. 250 pages were chosen as a reasonable amount, which in theory would lead to a raw dataset consisting of 6000 actors, but in practice ended up being 5564 actors. This would be the first dataframe \"df_actors\". Since it wasn't possible to use filters directly in the API-call, data processing was performed after the data was collected. The information about these actors that was collected were the attributes: name, id, gender, birthday, place_of_birth and popularity. In this project the focus is on recent actors, and therefore actors who passed away before 2010 are not part of the dataframe.\n",
    "\n",
    "Initially, all rows with missing values in the attributes of interest were removed and duplicate rows were removed. Since all the collected information about the actors is crucial for the analysis, this seemed a reasonable part of the cleaning. \n",
    "\n",
    "Using the retrieved actor IDs, it was now possible to get a complete list of the movies that each actor has played a role in as well as the movie ID - again using the TMDB API. This also laid the foundation for the second dataframe \"df_movies\" with the purpose of being our own little movie-database containing information about all the movies that the actors in the \"df_actors\" dataframe have been part of. Before any processing, the dataframe included 67587 movies.\n",
    "\n",
    "With yet another API call using the found movie IDs, the desired information could be obtained, which consisted of the attributes: rating, popularity, genres, release date and movie abstract. After retrieval of movie information in the movies dataframe, all rows with missing values in any of the attributes were removed. Furthermore, it was decided for this project to only focus on \"recent\" movies, which in this case is defined as movies from 2010 and until the date of collection (08-05-2024). After this processing the movie dataset consisted of 26494 movies. This was also done to avoid having a too dense network later.\n",
    "\n",
    "The outfiltered movies in the movies dataframe, were also removed from the actor dataframe, so the two dataframes would be consistent. After this filtering, actors with an empty movie list were removed, which cut down the number of actors to exactly 5100 in the actors dataframe.\n",
    "\n",
    "Through further investigation of the actors dataframe, it was found that the \"birthplace\" column needed processing. Firstly, the birthplace was reduced to only include the country of birth for the actors, but it was also found that the same countries were spelled differently. The country could be in different languages, symbols, etc., which could introduce misleading findings. The birthplace column was normalized so there would be no double instances of a country of birth. \n",
    "\n",
    "It was in our interest to select a main genre for each actor. Each movie has a list of given genres, and by finding the genre that appeared the most in the movies that the actor has been part of, a main genre for an actor was found. One issue that had to be adressed was regarding the \"Drama\" genre, where the problem was that a significant amoutn of movies were partially catogarized as \"Drama\". The internet defines the drama genre as movies that invokes emotion and features character development, which could characterise almost all movies. To overcome this challenge without removing the drama genre entirely, the drama category is removed from the genre list if there is more than 1 other category. In this way it is avoided that all actors would be \"Drama\" actors. \n",
    "\n",
    "Finally, from the TMDB API there is no direct actor rating measure, which gave us the idea to create our own measure. This measure was created as the mean of the ratings from all the movies the actor has acted in, of the more recent movies (2010-2024). \n",
    "\n",
    "Now that the data has been cleaned and preprocessed, it was time to create the actual network. The edges of the network were weighted by the number of times two actors had collaborated. This led to a very dense network, and therefore a threshold was set, so that there would only be an edge if the actors had collaborated at least twice. The final network consisted of the 5100 actors/nodes with 23736 edges. In the network there are 1680 isolated nodes, which refers to popular actors who have been part of movies since 2010, but not with anyone in this network. The largest component of the network consists of 3341 nodes and the network has a density of 0.0018. \n",
    "\n",
    "* Write a short section that discusses the dataset stats\n",
    "\n",
    "This project is based on two datasets, one for actor information and the other for movie information.\n",
    "These datasets have the following stats:\n",
    "\n",
    "Actor dataframe: 5100 rows of size 4.134 KB\n",
    "\n",
    "Movie Dataframe: 30086 rows of size 10.905 KB\n",
    "\n",
    "From these a network was made of actor collaborations, with the following stats:\n",
    "5100 nodes and 23736 edges, of size 2.388 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval and cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "import json\n",
    "from threading import Lock\n",
    "import matplotlib.cm as cm\n",
    "import pycountry\n",
    "import re\n",
    "import netwulf as nw\n",
    "import matplotlib.patches as mpatches\n",
    "import community.community_louvain as community_louvain\n",
    "from IPython.display import Image, display, HTML\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "from nltk import bigrams\n",
    "import collections\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "api_key = \"f5813332cb558d374cbcb057ea2fc48b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used to make API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "lock = Lock()\n",
    "\n",
    "def movie_title_and_IDs_from_actor_ID(actor_id, session):\n",
    "    global counter\n",
    "    url = f\"https://api.themoviedb.org/3/person/{actor_id}/movie_credits?api_key={api_key}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    with lock:\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Processed {counter} actors\")\n",
    "    # Return the whole movie dictionary, not just the title\n",
    "    return [movie['title'] for movie in data['cast']], [movie['id'] for movie in data['cast']]\n",
    "\n",
    "def actor_info_from_page(page, session):\n",
    "    url = f\"https://api.themoviedb.org/3/person/popular?api_key={api_key}&page={page}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "    people = []\n",
    "    for person in data['results']:\n",
    "        person_url = f\"https://api.themoviedb.org/3/person/{person['id']}?api_key={api_key}\"\n",
    "        person_response = session.get(person_url)\n",
    "        person_data = person_response.json()\n",
    "        if person_data['deathday'] is not None:\n",
    "            if person_data['deathday'] < '2010-01-01':\n",
    "                continue\n",
    "        people.append((person['name'], person['id'], person_data['gender'], person_data['birthday'], person_data['place_of_birth'], person_data['popularity']))\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "lock = Lock()\n",
    "\n",
    "def fetch(session, url):\n",
    "    global counter\n",
    "    future = session.get(url, headers=headers)\n",
    "    with lock:\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print(f\"Processed {counter} movies\")\n",
    "    \n",
    "    return future\n",
    "\n",
    "def movie_info_from_movie_ID(urls):\n",
    "    with FuturesSession() as session:\n",
    "        futures = [fetch(session, url) for url in urls]\n",
    "        responses = [future.result().json() for future in tqdm(futures, total=len(futures))]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve actor information (name, id, gender, birthday, birthplace, popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        pages = list(range(1, 301))\n",
    "        fetch_page_with_session = partial(actor_info_from_page, session=session)\n",
    "        people = list(executor.map(fetch_page_with_session, pages))\n",
    "\n",
    "actor_names, actor_ids, actor_genders, actor_birthdays, actor_birthplaces, actor_popularities = zip(*itertools.chain(*people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Actor Dataframe and initial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with 'actors' column\n",
    "df_actors = pd.DataFrame(actor_names, columns=['actor'])\n",
    "\n",
    "# Add 'ids', 'genders', and 'birthplaces' columns to the DataFrame\n",
    "df_actors['actor_id'] = actor_ids\n",
    "df_actors['gender'] = actor_genders\n",
    "df_actors['age'] = actor_birthdays # This is not the age, but the birthday\n",
    "df_actors['birthplace'] = actor_birthplaces\n",
    "df_actors['popularity'] = actor_popularities\n",
    "\n",
    "# Change birthplaces so that it only contains the country (text after the last comma)\n",
    "df_actors['birthplace'] = df_actors['birthplace'].str.split(',').str[-1]\n",
    "\n",
    "# Change birthday to age\n",
    "df_actors['age'] = pd.to_datetime(df_actors['age'], errors='coerce')\n",
    "df_actors['age'] = (pd.to_datetime('today') - df_actors['age']).dt.days // 365\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_actors.dropna(inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df_actors.drop_duplicates(subset='actor_id', inplace=True)\n",
    "#reset index\n",
    "df_actors.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve movie titles and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch movies for each actor and add them to 'movies' and 'movie_IDs' columns\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    movie_titles_and_ids_from_actor_ID_with_session = partial(movie_title_and_IDs_from_actor_ID, session=session)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        movies_and_ids = list(executor.map(movie_titles_and_ids_from_actor_ID_with_session, df_actors['actor_id']))\n",
    "\n",
    "# Add 'movies' column to df_actors\n",
    "df_actors['movies'] = [x[0] for x in movies_and_ids]\n",
    "df_actors['movie_IDs'] = [x[1] for x in movies_and_ids]\n",
    "\n",
    "# Flatten movies_and_ids into two separate lists\n",
    "movies = [item for sublist in [x[0] for x in movies_and_ids] for item in sublist]\n",
    "ids = [item for sublist in [x[1] for x in movies_and_ids] for item in sublist]\n",
    "\n",
    "# Convert the lists into a list of dictionaries\n",
    "movies_and_ids_dict = [{'movie': movie, 'movie_ID': id} for movie, id in zip(movies, ids)]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_movies = pd.DataFrame(movies_and_ids_dict)\n",
    "df_movies = df_movies.drop_duplicates(subset='movie_ID')\n",
    "df_movies.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect movie information trough API (rating, popularity, genre, release date, abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the URLs\n",
    "urls = [f\"https://api.themoviedb.org/3/movie/{id}?api_key={api_key}\" for id in df_movies[\"movie_ID\"]]\n",
    "print('urls are prepared')\n",
    "\n",
    "# Fetch all responses\n",
    "responses = movie_info_from_movie_ID(urls)\n",
    "print('all responses are fetched')\n",
    "\n",
    "# Process the responses\n",
    "for i, response in enumerate(responses):\n",
    "    # if i % 100 == 0:\n",
    "    #     print(f\"Processing response {i+1}/{len(responses)}\")\n",
    "    if isinstance(response, Exception):\n",
    "        print(f\"Error: {response}\")\n",
    "        continue  # Skip this response\n",
    "    # Process the response here\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "ratings = []\n",
    "popularities = []\n",
    "genres = []\n",
    "release_dates = []\n",
    "abstracts = []\n",
    "\n",
    "# Process the responses one by one\n",
    "for data in tqdm(responses):\n",
    "    ratings.append(data.get('vote_average'))\n",
    "    popularities.append(data.get('popularity'))\n",
    "    genres.append([genre['name'] for genre in data.get('genres', [])])\n",
    "    release_dates.append(data.get('release_date'))\n",
    "    abstracts.append(data.get('overview'))\n",
    "\n",
    "# Assign the lists to the DataFrame columns\n",
    "df_movies['rating'] = ratings\n",
    "df_movies['popularity'] = popularities\n",
    "df_movies['genres'] = genres\n",
    "df_movies['release_date'] = release_dates\n",
    "df_movies['abstract'] = abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing of Movies Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_movies.drop_duplicates(subset='movie', inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_movies.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with empty lists in 'genres' column\n",
    "df_movies = df_movies[df_movies['genres'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Remove rows with empty release dates\n",
    "df_movies = df_movies[df_movies['release_date'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if abstract is missing\n",
    "df_movies = df_movies[df_movies['abstract'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if rating is missing\n",
    "df_movies = df_movies[df_movies['rating'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Drop row if popularity is missing\n",
    "df_movies = df_movies[df_movies['popularity'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Reset index\n",
    "df_movies.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove old and future movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes so that each actor is associated with the movies they have acted in\n",
    "df_actors_filtered = df_actors.copy()\n",
    "df_movies_filtered = df_movies.copy()\n",
    "df_movies_filtered.rename(columns={'movie_ID': 'movie_IDs'}, inplace=True)\n",
    "df_movies_filtered.rename(columns={'popularity': 'movie_popularity'}, inplace=True)\n",
    "df_actors_filtered.drop(columns=['movies'], inplace=True)\n",
    "\n",
    "df_actors_filtered = df_actors_filtered.explode('movie_IDs').reset_index(drop=True)\n",
    "\n",
    "df_actors_movies = df_actors_filtered.merge(df_movies_filtered, on='movie_IDs', how='inner')\n",
    "\n",
    "# Remove all rows where the release date is before 2010 or after 2024 (to avoid movies that have not been released yet)\n",
    "df_actors_movies = df_actors_movies[(df_actors_movies['release_date'] >= '2010-01-01') & (df_actors_movies['release_date'] <= '2024-05-08')]\n",
    "\n",
    "# Collapse the the actors in the actor column so there is only one row per actor and the movies and movie_IDs are stored in lists\n",
    "df_actors_filtered = df_actors_movies.groupby('actor').agg({'actor_id': 'first',\n",
    "                                                             'gender': 'first',\n",
    "                                                             'birthplace': 'first',\n",
    "                                                             'age': 'first',\n",
    "                                                             'popularity': 'first',\n",
    "                                                             'movie': list, \n",
    "                                                             'movie_IDs': list}).reset_index()\n",
    "\n",
    "df_movies_filtered = df_actors_movies.drop_duplicates(subset=['movie']).groupby('movie').agg({'movie_IDs': 'first',\n",
    "                                                                                               'rating': 'first',\n",
    "                                                                                               'popularity': 'first',\n",
    "                                                                                               'genres': 'first',\n",
    "                                                                                               'release_date': 'first',\n",
    "                                                                                               'abstract': 'first'}).reset_index()\n",
    "\n",
    "df_actors_filtered.rename(columns={'movie': 'movies'}, inplace=True)\n",
    "df_movies_filtered.rename(columns={'movie_IDs': 'movie_ID'}, inplace=True)\n",
    "\n",
    "# Remove actors with empty lists of movies\n",
    "df_actors_filtered = df_actors_filtered[df_actors_filtered['movies'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the birthplace column, to avoid having the same country spelled differently and removing symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean birthplace column in actor dataframe\n",
    "len_bef_clean = len(df_actors_filtered['birthplace'].unique())\n",
    "\n",
    "country_names = [country.name for country in pycountry.countries]\n",
    "\n",
    "def normalize_country_name(name):\n",
    "    # strip name\n",
    "    name = name.strip()\n",
    "    # replace '.' with ''\n",
    "    name = name.replace('.', '').replace(']', '')\n",
    "\n",
    "    if \"UK\" in name or \"İngiltere\" in name:\n",
    "        return \"United Kingdom\"\n",
    "    try:\n",
    "        # Try to get the country object\n",
    "        country = pycountry.countries.get(name=name)\n",
    "        if country is not None:\n",
    "            # If the country object is found, return the official name\n",
    "            return country.name\n",
    "        else:\n",
    "            # If the country object is not found, try to find it by its common name\n",
    "            country = pycountry.countries.search_fuzzy(name)\n",
    "            return country[0].name\n",
    "    except LookupError:\n",
    "        # Standardizing names\n",
    "        for country_name in country_names:\n",
    "            if country_name in name:\n",
    "                name = country_name\n",
    "\n",
    "        # Fixing abbreviations and wird instances\n",
    "        if \"Russia\" in name:\n",
    "            return \"Russian Federation\"\n",
    "        elif \"Türkiye\" in name or \"Turkey\" in name:\n",
    "            return \"Türkiye\"\n",
    "        elif \"USA\" in name or \" US\" in name or \"United States\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Korea\" in name:\n",
    "            return \"Korea, Republic of\"\n",
    "        elif \"Czech\" in name:\n",
    "            return \"Czechia\"\n",
    "        # Hardcoded, could use package to translate, maybe not necessary, few occurences\n",
    "        elif \"TX\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Frankrike\" in name:\n",
    "            return \"France\"\n",
    "        elif \"Afrique du Sud\" in name:\n",
    "            return \"South Africa\"\n",
    "        elif \"Irlanda\" in name:\n",
    "            return \"Ireland\"\n",
    "        elif \"中国\" in name or \"中华民国\" in name or \"重庆\" in name or \"南京\" in name:\n",
    "            return \"China\"\n",
    "        \n",
    "        \n",
    "        # Updating from old names\n",
    "        if \"now\" in name:\n",
    "            match = re.search(r'\\[now (.*?)', name)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        # If the country is not found, return the original name\n",
    "        return name\n",
    "\n",
    "# Normalize the country names in the DataFrame\n",
    "df_actors_filtered['birthplace'] = df_actors_filtered['birthplace'].apply(normalize_country_name)\n",
    "df_actors_filtered = df_actors_filtered.dropna(subset=['birthplace'])\n",
    "\n",
    "len_aft_clean = len(df_actors_filtered['birthplace'].unique())\n",
    "# print(\"length before cleaning\", len_bef_clean)\n",
    "# print(\"length after cleaning\", len_aft_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the main genre for the popular actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the genre counts and main genre of an actor\n",
    "def find_genres_and_main_genre(movies_list, remove_drama=False): # nearly half of all movies have drama as genre, so not descriptive\n",
    "    # Get the genres of the movies\n",
    "    genres = [genre for movie in movies_list for genre in movie_genres.get(movie, [])]\n",
    "    # Count the frequency of each genre\n",
    "    genre_counts = pd.Series(genres).value_counts()\n",
    "    # Remove the 'Drama' genre if specified\n",
    "    if remove_drama and len(genre_counts) > 2:\n",
    "        genre_counts.drop('Drama', errors='ignore', inplace=True)\n",
    "        #genre_counts.drop('Thriller', errors='ignore', inplace=True)\n",
    "    \n",
    "    # Return the genre counts as a dictionary and the main genre\n",
    "    return genre_counts.to_dict(), genre_counts.idxmax() if not genre_counts.empty else None\n",
    "\n",
    "movie_genres = df_movies_filtered.set_index('movie')['genres'].to_dict()\n",
    "# Apply the function to the 'movies' column of the actors DataFrame and create two new columns\n",
    "df_actors_filtered['genres'], df_actors_filtered['main_genre'] = zip(*df_actors_filtered['movies'].apply(find_genres_and_main_genre, remove_drama=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the average rating and popularity of an actor\n",
    "def find_actor_rating(movies_list):\n",
    "    # Get the ratings for each movie\n",
    "    ratings = [movie_ratings[movie] for movie in movies_list if movie in movie_ratings]\n",
    "    avg_rating = np.mean(ratings)\n",
    "\n",
    "    return ratings, avg_rating\n",
    "\n",
    "movie_ratings = df_movies_filtered.set_index('movie')['rating'].to_dict()\n",
    "\n",
    "# Apply the function to the 'movies' column of the actors DataFrame and create two new columns\n",
    "df_actors_filtered['ratings'], df_actors_filtered['avg_rating'] = \\\n",
    "        zip(*df_actors_filtered['movies'].apply(find_actor_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the final dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv files\n",
    "df_actors_filtered.to_csv('data/actors.csv', index=False)\n",
    "df_movies_filtered.to_csv('data/movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of top 10 actor birthplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axes in a 1x2 grid\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# First subplot: Top 10 Birthplaces of Actors\n",
    "birthplace_counts = df_actors_filtered['birthplace'].value_counts()\n",
    "birthplace_percentages = (birthplace_counts / df_actors_filtered['birthplace'].count()) * 100\n",
    "top_5_birthplaces = birthplace_percentages[:10]\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(top_5_birthplaces)))\n",
    "top_5_birthplaces.plot(kind='bar', color=colors, edgecolor='black', ax=axs[0])\n",
    "axs[0].set_title('Top 10 Birthplaces of Actors')\n",
    "axs[0].set_xlabel('Birthplace')\n",
    "axs[0].set_ylabel('Percentage of Actors (%)')\n",
    "\n",
    "# Second subplot: Top 10 Main Genres of Actors\n",
    "main_genre_counts = df_actors_filtered['main_genre'].value_counts()\n",
    "main_genre_percentages = (main_genre_counts / df_actors_filtered['main_genre'].count()) * 100\n",
    "top_5_main_genres = main_genre_percentages[:10]\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(top_5_main_genres)))\n",
    "top_5_main_genres.plot(kind='bar', color=colors, edgecolor='black', ax=axs[1])\n",
    "axs[1].set_title('Top 10 Main Genres of Actors')\n",
    "axs[1].set_xlabel('Main Genre')\n",
    "axs[1].set_ylabel('Percentage of Actors (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The number of unique birthplaces:\",len(df_actors_filtered['birthplace'].unique()))\n",
    "birthplaces_with_count_1 = birthplace_counts[birthplace_counts == 1]\n",
    "print(\"Number of countries only associated with One actor in our dataset\",len(birthplaces_with_count_1))\n",
    "\n",
    "print(\"\\nThe number of unique main genres:\", len(df_actors_filtered['main_genre'].unique()))\n",
    "main_genres_with_count_1 = main_genre_counts[main_genre_counts == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: TMDB is very biased towards USA with their popular actors data that we have retrieved (over 50% of the total number of actors). Therefore most of the movies are american and the actors from different countries are biased towards the american movie industry.\n",
    "\n",
    "* It is also worth noting that 43 of the 116 unique birthplaces in our dataset is only associated with one actor, which is also a factor causing the unexpected low assortativity for the birthplace attribute. \n",
    "\n",
    "* To fix this or make it more representative, we could webscrape from other websites to make a less biased dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes from csv files\n",
    "df_actors_filtered = pd.read_csv('data/actors.csv', converters={'movies': ast.literal_eval, 'movie_IDs': ast.literal_eval, 'ratings': ast.literal_eval, 'genres': ast.literal_eval})\n",
    "df_movies_filtered = pd.read_csv('data/movies.csv', converters={'genres': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "def create_network(df):\n",
    "    movie_to_actors = defaultdict(list)\n",
    "\n",
    "    for _ , row in df.iterrows():\n",
    "        actor = row['actor']\n",
    "        movies = row['movies']\n",
    "        for movie in movies:\n",
    "            movie_to_actors[movie].append(actor)\n",
    "\n",
    "    edges = defaultdict(int)\n",
    "    for actors in movie_to_actors.values():\n",
    "        for pair in itertools.combinations(sorted(actors), 2):\n",
    "            edges[pair] += 1\n",
    "\n",
    "    # Step 2: Create the Graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with attributes\n",
    "    for _ , row in df.iterrows():\n",
    "        G.add_node(row['actor'], gender=row['gender'], age=row['age'], birthplace=row['birthplace'], main_genre=row['main_genre'],\n",
    "                avg_rating=row['avg_rating'], popularity=row['popularity'])\n",
    "\n",
    "    # Add edges with weights\n",
    "    for edge, weight in edges.items():\n",
    "\n",
    "    # Adding only the edges with weight >= 2 to get less dense graph\n",
    "        if weight >= 2:\n",
    "            G.add_edge(*edge, weight=weight)\n",
    "\n",
    "    return G\n",
    "\n",
    "G = create_network(df_actors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network to json()\n",
    "data = nx.node_link_data(G)\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open('data/actor_network.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of nodes and edges\n",
    "N = G.number_of_nodes()\n",
    "L = G.number_of_edges()\n",
    "isolates = list(nx.isolates(G))\n",
    "components = list(nx.connected_components(G))\n",
    "largest_component = max(components, key=len)\n",
    "edge_density = nx.density(G)\n",
    "\n",
    "print(\"Number of nodes: \", N)\n",
    "print(\"Number of edges: \", L)\n",
    "print(\"Number of isolates: \", len(isolates))\n",
    "print(\"Number of components: \", len(components))\n",
    "print(\"Number of nodes in the largest component: \", len(largest_component))\n",
    "print(\"Edge density: \", edge_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, theory and analysis\n",
    "\n",
    "* Describe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\n",
    "\n",
    "Degree analysis: \n",
    "\n",
    "In the first part of the network analysis, the focus is on node degrees. The degree of a node is the amount of edges that a node has in the network. Since this network is a undirected network, a weight can be added to the edges, so that the same edges are still counted multiple times. The degree distribution is found and illustrated to see whether the distribution is heavy-tailed, meaning that most nodes have few connections in the network and that there are hubs with a significantly degree than the average and median of the network. \n",
    "\n",
    "Regime:\n",
    "\n",
    "Looking into which regime the network belongs is an important analysis for understanding and characterizing the network. By doing simple calculations the regime and characteristics that follow can be determined. \n",
    "\n",
    "Visualization of the network:\n",
    "\n",
    "More realizations about the network can be obtained by simply visualizing the network. Firstly, the naturally formed communities and the specific hubs in the network can be identified by sizing the nodes by degree. Furthermore, the nodes can be colored according to a specific node attribute, e.g. country of birth, to see whether there are any connections in the network based on this attribute. \n",
    "\n",
    "Assortativity:\n",
    "\n",
    "The assortativity measure refers to the tendency of nodes to link to other nodes with the same (or similar for numeric values) attribute values, such as degree, age, gender etc. The assortativity coefficient ranges from -1 (perfect disassortative) to 1 (perfect assortative). A positive assortativity means that nodes with similar attribute values tend to connect, and a negative assortativity coefficient means that nodes with dissimilar values tend to form connections. As an example if the assortativity attribute is gender, in a perfect assortative network males would only connect to other males, and in a perfect dissasortative network males would only connect to females (in binary gendered network). \n",
    "\n",
    "Closeness/eigenvector centrality:\n",
    "\n",
    "The closeness and eigenvector centrality measure is used to identify the most central nodes in the network. The most central nodes according to the closeness measure, are the nodes that have the longest reach within the fewest edges in the network. Information can flow through these nodes to any other nodes in the network through the shortest possible paths.\n",
    "On the other hand, the eigenvector centrality measure identifies nodes that are not just central because they are connected to many other nodes, but because they are connected to other nodes that are highly connected. \n",
    "These measures can be used in the actor network to identify the most influential actors. \n",
    "\n",
    "\n",
    "* How did you use the tools to understand your dataset?\n",
    "Degree analysis:\n",
    "\n",
    "After the network was created, the degrees of the nodes in the network were investigated. The average, median, mode, minimum and maximum degree were computed for both the weighted and unweighted network. Furthermore, the degree distribution of the unweighted network is plotted. These metrics and illustrations give an understanding of whether the network contains hubs (nodes with significantly more edges) and whether the network is heavy-tailed distributed.\n",
    "\n",
    "Next, the highest degree nodes were found. The top 5 actors with highest unweighted degree illustrate the popular actors that have most unique collaborations, where the weighted degree illustrates the popular actors with with most collaborations in the popular actor network, this being both unique and non-unique collborations. By looking the at most repeated edges (edges with highest weight), the \"best friends\" of the network were found. \n",
    "\n",
    "Regime:\n",
    "\n",
    "To get a better overall understanding of the characteristics of the actor network, the regime that this network falls into was found through some simple calculations. By calculating the average degree and comparing it to the logarithm to the number of nodes in the network, it was found that the network falls into the connected regime. This means that most actors in the network are connected in one giant component. \n",
    "\n",
    "Visualization:\n",
    "\n",
    "Now that it is known that the network is connected, the network could be further analyzed by visualizing the network. Using the Netwulf package the network was visualized and the actors were colored according to their birthplace and the node size was set according to their weighted degree. In this way, it was possible to see whether interesting clusters could be found within the network. \n",
    "\n",
    "Assortativity:\n",
    "To further analyze the network and understand the patterns, the specific node attributes were at focus. Calculating the assortativity for the most central node attributes, could give insights into whether these attributes influence the formation of groups within the network. The attributes in focus were the numeric attributes: degree, age, rating and popularity, and the categorical: genre, birthplace and gender. \n",
    "\n",
    "Closeness/eigenvector centrality:\n",
    "The closeness and eigenvector centrality is used in our network analysis to identify the most central actors in the network. Calculating both measures provides a better overview of central nodes, not only in terms of ability to quickly reach other nodes, but also in terms of their influence based on connections to other central actors. \n",
    "\n",
    "* Talk about how you’ve worked with text, including regular expressions, unicode, etc.\n",
    "\n",
    "The text data of our project consists of short movie abstracts (average length of around 50 words). The first step was to clean and tokenize the abstracts. We do this with our tokenize_and_clean function, which standardize the text in terms of excluding symbols, numbers and punctuation and converting everything to lowercase. After the cleaning the text gets tokenized, the tokens get stemmed and the English stop-words are removed. A new column in the movies dataframe is created, called “tokens” which is a list of tokens for each movie abstract. As it is of interest to address potential collocation issues, bigram work is done to consider words together that tend to appear together more often than by chance, to retain their original meaning. This is done by extracting all unique bigrams from the tokens column that is exploded, and then comparing the observed contingency table with the expected contingency table. To determine if two words are a collocation a Chi-Squared test is used. Then for each pair of bigrams in the unique bigrams list if the bigram has a frequency above 10 and a p-value from the test under 0.001, it is put into a collocation dictionary. The reason behind this relatively low frequency threshold is because of the short sizes of the abstracts, as if higher too many collocations were filtered out. The tokens are then recomputed with the tokenize_and_clean function, but the tokenizer now considers the collocations, and binds the two words with an underscore. \n",
    "\n",
    "After partitioning the network into communities it is of interest to characterize and identify communities through text analysis, such as TF-IDF based word-clouds. A high TF-IDF value is a good measure to base the word-clouds as it will make the words that stand out in a specific community or topic are emphasized in the word cloud, providing insights into the distinctive language and themes within that community. It is only computed for the top 9 communities in terms of average rating, as these are the only ones to be visualized through the word-clouds. Therefore the IDF is computed for each term unique term in the abstracts of the top 9 as the logarithm of the ratio between the total number of abstracts in the top 9 communities and the number of abstracts containing the term. And the TF is then the frequency of the terms in the 9 communities. By doing so each term and the associated TF-IDF value of the top communities are collected in a dictionary, for our community analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degrees and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute node degrees\n",
    "degrees = dict(G.degree())\n",
    "weighted_degrees = dict(G.degree(weight='weight'))\n",
    "\n",
    "# Compute the specificied values\n",
    "average_degree = np.mean(list(degrees.values()))\n",
    "median_degree = np.median(list(degrees.values()))\n",
    "try:\n",
    "    mode_degree = mode(list(degrees.values())) #Degree value that occurs with highest frequency among the nodes\n",
    "except:\n",
    "    mode_degree = \"No unique mode\"\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "# Same calculations but for STRENGTH (WEIGHTED DEGREE)\n",
    "average_weighted_degree = np.mean(list(weighted_degrees.values()))\n",
    "median_weighted_degree = np.median(list(weighted_degrees.values()))\n",
    "try:\n",
    "    mode_weighted_degree = mode(list(weighted_degrees.values()))\n",
    "except:\n",
    "    mode_weighted_degree = \"No unique mode\"\n",
    "min_weighted_degree = min(weighted_degrees.values())\n",
    "max_weighted_degree = max(weighted_degrees.values())\n",
    "\n",
    "print(\"Degree Analysis:\")\n",
    "print(f\"Average Degree: {average_degree}\")\n",
    "print(f\"Median Degree: {median_degree}\")\n",
    "print(f\"Mode Degree: {mode_degree}\")\n",
    "print(f\"Minimum Degree: {min_degree}\")\n",
    "print(f\"Maximum Degree: {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree Analysis:\")\n",
    "print(f\"Average Weighted Degree: {average_weighted_degree}\")\n",
    "print(f\"Median Weighted Degree: {median_weighted_degree}\")\n",
    "print(f\"Mode Weighted Degree: {mode_weighted_degree}\")\n",
    "print(f\"Minimum Weighted Degree: {min_weighted_degree}\")\n",
    "print(f\"Maximum Weighted Degree: {max_weighted_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the edges being weighted, the average, median and max degree increases significantly, which is expected as popular actors may have a tendency of collaborating multiple times with other popular actors. The mode degree remains at 0, which is caused by the high number of isolates in the network. It is also worth noting, that purely based on the average and median degree, it can be seen that both the unweighted and weighted degree distribution are heavy-tailed as the median is significantly smaller than the average degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree distribution\n",
    "degree_sequence = sorted([d for n, d in G.degree()], reverse=False)  # sort in ascending order\n",
    "degree_count = nx.degree_histogram(G)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# degree distribution\n",
    "axs[0].bar(range(len(degree_count)), degree_count, align='center')\n",
    "axs[0].axvline(average_degree, color='r', linestyle='-', label='Average Degree')\n",
    "axs[0].set_xlabel('Degree')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_title('Degree Distribution')\n",
    "axs[0].legend()\n",
    "\n",
    "# loglog degree distribution\n",
    "axs[1].bar(range(len(degree_count)), degree_count, align='center')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('Degree')\n",
    "axs[1].set_ylabel('Count')\n",
    "axs[1].set_title('Degree Distribution (log scale)')\n",
    "axs[1].axvline(average_degree, color='r', linestyle='-', label='Average Degree')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unweighted degree distribution is plotted in normal scale and log-log-scale. On normal scale, it is clear to see the heavy-tailed distribution, which is further established through the linearity on the log-log-scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most collaborative pairs in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the edges by weight in descending order\n",
    "sorted_edges = sorted(G.edges.data(), key=lambda x: x[2]['weight'], reverse=True)\n",
    "most_connected_actors = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "most_connected_actors_weighted = sorted(weighted_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 5 actors with the highest degree:\")\n",
    "for i, actor in enumerate(most_connected_actors[:5], start=1):\n",
    "    print(f\"{i}: Degree: {actor[1]},\\t ({G.nodes[actor[0]]['main_genre']})\\t {actor[0]}\")\n",
    "\n",
    "print(\"\\nTop 5 actors with the highest weighted degree:\")\n",
    "for i, actor in enumerate(most_connected_actors_weighted[:5], start=1):\n",
    "    print(f\"{i}: Degree: {actor[1]},\\t ({G.nodes[actor[0]]['main_genre']})\\t {actor[0]}\")\n",
    "\n",
    "print(\"\\nTop 5 most important edges:\")\n",
    "for i, edge in enumerate(sorted_edges[:5], start=1):\n",
    "    print(f\"{i}: Degree: {edge[2]['weight']}, ({G.nodes[edge[0]]['main_genre']} - {G.nodes[edge[1]]['main_genre']}) \\t {edge[0]} - {edge[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the unweighted network, Samuel L. Jackson was at the top and is thus the actor with most unique collaborations in the popular actor network. For the weighted network, Grey DeLisle was at the top meaning she has the most total collaborations with popular actors in the network.\n",
    "The two actors with the highest weighted edge were Frank Welker and Grey DeLisle, who are best known for being the voices of two Scooby Doo characters. Interestingly, three of the five pairs in the top five are three wrestling pairs who are all known from the American WWE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the regime of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability that makes the expected number of edges equal to the actual number of edges in the graph.\n",
    "p = 2*L/(N*(N-1))\n",
    "print(\"Prob:\",p)\n",
    "\n",
    "# Compute the natural logarithm of N\n",
    "ln_N = np.log(N)\n",
    "\n",
    "print(f\"k: {average_degree:.2f} > ln(N): {ln_N:.2f}\\np: {p:.5f} > ln(N)/N: {ln_N/N:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the average degree is 9.31 and the logarithm of N is 8.54 (<k> > ln(N)), the network must fall into the connected regime. It is therefore above the critical threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the network according to birthplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with netwulf\n",
    "G_plot_nation = G.copy()\n",
    "\n",
    "for k, v in G_plot_nation.nodes(data=True):\n",
    "    v['group'] = v['birthplace']; del v['birthplace']\n",
    "\n",
    "network, config = nw.visualize(G_plot_nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the color of all nodes in the network and find the top 10 most used colors\n",
    "node_colors = [node['color'] for node in network['nodes']]\n",
    "\n",
    "# Count the frequency of each color\n",
    "color_counts = pd.Series(node_colors).value_counts()\n",
    "\n",
    "# Get the top 10 most used colors\n",
    "top_10_colors = color_counts.head(10)\n",
    "\n",
    "# Create color dictionary\n",
    "color_dict = {}\n",
    "\n",
    "# Go through the nodes and assign the color to the birthplace\n",
    "for color in top_10_colors.index:\n",
    "    for node in network['nodes']:\n",
    "        if node['color'] == color:\n",
    "            color_dict[color] = df_actors_filtered[df_actors_filtered['actor']==node['id']]['birthplace'].values[0]\n",
    "            break\n",
    "\n",
    "# Print the color dictionary\n",
    "for color, birthplace in color_dict.items():\n",
    "    print(f\"Color: {color}, Birthplace: {birthplace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a color legend for the countries\n",
    "patches = [mpatches.Patch(color=color, label=f\"{color_dict[color]}\") for color in color_dict]\n",
    "\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.legend(handles=patches, loc='center', frameon=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = nw.draw_netwulf(network)\n",
    "plt.legend(handles=patches, loc='upper left', frameon=False)\n",
    "fig.set_size_inches(12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "# fig.savefig('figures/country_graph.png')\n",
    "\n",
    "# Load the saved png-file \"country_graph.png\" and plot it in the notebook\n",
    "Image(filename='figures/country_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of the network shows, as expected, that there are a lot of large american actors in the network, who seem to clustered very central in the network. Furthermore, the actors from the U.K. are primarily clustered in the bottom left side of the giant middle component. But more interestingly, a very clear cluster is formed at the top of the plot, where the all Japanese actors are very closely linked but quite isolated from the giant component in the middle. Another interesting cluster is the Korean cluster to the left of the giant component. These actors are also isolated from the rest of the nodes in the giant component. \n",
    "\n",
    "It seems reasonable to have clusters like the Japanese and Korean cluster, as these actors most likely work on movies in their respective native languages and thus with other actors from these countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree assortativity coefficient\n",
    "degree_assortativity = nx.degree_assortativity_coefficient(G)\n",
    "\n",
    "# Calculate the attribute assortativity coefficient\n",
    "country_assortativity = nx.attribute_assortativity_coefficient(G, 'birthplace')\n",
    "\n",
    "gender_assortativity = nx.attribute_assortativity_coefficient(G, 'gender')\n",
    "\n",
    "# Calculate the numeric assortativity coefficient\n",
    "age_assortativity = nx.numeric_assortativity_coefficient(G, 'age')\n",
    "\n",
    "# Calcualte the assortativity for main_genre\n",
    "main_genre_assortativity = nx.attribute_assortativity_coefficient(G, 'main_genre')\n",
    "\n",
    "rating_assortativity = nx.numeric_assortativity_coefficient(G, 'avg_rating')\n",
    "popularity_assortativity = nx.numeric_assortativity_coefficient(G, 'popularity')\n",
    "\n",
    "print(f\"Categorical:\\nGenre assortativity: {main_genre_assortativity}, \\nBirth place assortativity: {country_assortativity}, \\nGender assortativity: {gender_assortativity}\")\n",
    "\n",
    "print(f\"\\nNumeric:\\nDegree assortativity: {degree_assortativity}, \\nAge assortativity: {age_assortativity}, \\nRating assortativity: {rating_assortativity}, \\nPopularity assortativity: {popularity_assortativity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are both categorical and numeric attributes, they will be compared seperately. For the categorical attributes, the birth place assortativity is the highest at 0.47. This suggests that actors born in the same country are more likely to collaborate. The value of 0.47 makes sense, as movies of recent times typically features a diverse cast from different nations, implying that actors will not exclusively work with those from their own country. The genre assortativity is also positive at 0.34, again meaning that actors that work on similar types of movies are more likely to collaborate. Since the popular actors network has a gender assortativity close to 0, the connections and clusters are formed independently from gender. \n",
    "\n",
    "When looking at the numeric attributes, the rating is the most assortative attribute with a value of 0.45. This implies that in the popular actor network highly rated actors tend to work with other highly rated actors. The age assortativity is at 0.32, suggesting that the actor network might be segmented into groups based on age. While the degree and popularity assortativity are slightly positive, these attributes might not be the most influential factors in group formation within the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the closeness and eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the closeness centrality of the network\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "# Calculate the eigenvector centrality of the network\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Sort the actors according to the closeness centrality\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort the scientists according to the eigenvector centrality\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 10 most central actors\n",
    "most_central_actors_closeness = sorted_closeness_centrality[:10]\n",
    "# Find the 10 most central actors\n",
    "most_central_actors_eigenvector = sorted_eigenvector_centrality[:10]\n",
    "\n",
    "print(f'{\"Actor (Closeness Centrality)\":<30} {\"Score\":<10} | {\"Actor (Eigenvector Centrality)\":<30} {\"Score\"}')\n",
    "print('-'*80)\n",
    "for actor1, actor2 in zip(most_central_actors_closeness, most_central_actors_eigenvector):\n",
    "    print(f'{actor1[0]:<30} {actor1[1]:<10.3f} | {actor2[0]:<30} {actor2[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 actors according to the closeness and eigenvector centrality are completely different. Closeness centrality finds the most central actors as all English-speaking and very popular in the Western world. However, the eigenvector centrality shows a list of only Japanese actors, who are primarily grouped in a closely linked, isolated cluster in the network visualization. The reason that the eigenvector centrality prioritizes the Japanese actors is due to their strong connections within their cluster. As the actors are closely linked to each other, they all amplify the influence of each other. Even though these actors are quite isolated, their tight relations mean that they have a high degree of influence in their own cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the closeness and eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closeness centrality vs node degree to see if there is a correlation\n",
    "closeness_centrality_values = list(closeness_centrality.values())\n",
    "eigenvector_centrality_values = list(eigenvector_centrality.values())\n",
    "degree_values = list(dict(G.degree()).values())\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# plot the closeness centrality vs node degree to see if there is a correlation\n",
    "axs[0].scatter(degree_values, closeness_centrality_values, alpha=0.5)\n",
    "axs[0].set_xlabel('Node Degree')\n",
    "axs[0].set_ylabel('Closeness Centrality')\n",
    "axs[0].set_title('Closeness Centrality vs Node Degree')\n",
    "\n",
    "# plot the eigenvector centrality vs node degree to see if there is a correlation\n",
    "axs[1].scatter(degree_values, eigenvector_centrality_values, alpha=0.5)\n",
    "axs[1].set_xlabel('Node Degree')\n",
    "axs[1].set_ylabel('Eigenvector Centrality')\n",
    "axs[1].set_title('Eigenvector Centrality vs Node Degree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of the closeness and eigenvector centrality are challenging to interpret. In particular, the eigenvector centrality plot is unusual, as most of the nodes appear with a value close to 0. The top 10 actors according to the eigenvector centrality are only Japanese suggesting that all non-zero values represent Japanese actors. To verify this assumption, a new graph and plot are created without the Japanese actors below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_no_japan = df_actors_filtered[df_actors_filtered['birthplace'] != 'Japan']\n",
    "\n",
    "G_no_japan = create_network(df_actors_no_japan)\n",
    "\n",
    "# Calculate the closeness centrality of the network\n",
    "closeness_centrality = nx.closeness_centrality(G_no_japan)\n",
    "# Calculate the eigenvector centrality of the network\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G_no_japan)\n",
    "\n",
    "# Plot the closeness centrality vs node degree to see if there is a correlation\n",
    "closeness_centrality_values = list(closeness_centrality.values())\n",
    "eigenvector_centrality_values = list(eigenvector_centrality.values())\n",
    "degree_values = list(dict(G_no_japan.degree()).values())\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# plot the closeness centrality vs node degree to see if there is a correlation\n",
    "axs[0].scatter(degree_values, closeness_centrality_values, alpha=0.5)\n",
    "axs[0].set_xlabel('Node Degree')\n",
    "axs[0].set_ylabel('Closeness Centrality')\n",
    "axs[0].set_title('Closeness Centrality vs Node Degree (without Japan)')\n",
    "\n",
    "# plot the eigenvector centrality vs node degree to see if there is a correlation\n",
    "axs[1].scatter(degree_values, eigenvector_centrality_values, alpha=0.5)\n",
    "axs[1].set_xlabel('Node Degree')\n",
    "axs[1].set_ylabel('Eigenvector Centrality')\n",
    "axs[1].set_title('Eigenvector Centrality vs Node Degree (without Japan)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When removing the Japanese actors, the correlation between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used for tokenization of movie abstracts, including cleaning, stemming and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# Define a function to tokenize and clean text\n",
    "def tokenize_and_clean_text3(text, collocations={}, with_collocations=False): #takes text and dictionary of collocations\n",
    "    # Exclude URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Exclude mathematical symbols and numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Exclude punctuation and convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text).lower()\n",
    "\n",
    "    # Tokenize\n",
    "    if with_collocations:\n",
    "        tokenizer = MWETokenizer(list(collocations.keys()))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = tokenizer.tokenize(tokens)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # stem\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('english')]# TO REMOVE STOPWORDS \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First clean text\n",
    "df_movies_filtered['tokens'] = df_movies_filtered['abstract'].apply(lambda x: tokenize_and_clean_text3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store all bigrams\n",
    "all_bigrams = []\n",
    "\n",
    "# For each list of tokens in the 'tokens' column\n",
    "for tokens in df_movies_filtered['tokens']:\n",
    "    # Generate the list of bigrams\n",
    "    bigrams_list = list(bigrams(tokens))\n",
    "    # Add the bigrams to the list of all bigrams\n",
    "    all_bigrams.extend(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_bigrams = list(set(all_bigrams))\n",
    "tokens_list = list(df_movies_filtered['tokens'].copy().explode())\n",
    "\n",
    "# Create a Counter for bigrams and tokens\n",
    "bigrams_counter = collections.Counter(all_bigrams)\n",
    "tokens_counter = collections.Counter(tokens_list)\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# For each unique pair of words\n",
    "for w1, w2 in tqdm(all_unique_bigrams):\n",
    "    # number of times w1 and w2 appear together in all_bigrams\n",
    "    nii = bigrams_counter[(w1, w2)]\n",
    "    # number of times w1 appears without w2\n",
    "    noi = tokens_counter[w1] - nii\n",
    "    # number of times w2 appears without w1\n",
    "    nio = tokens_counter[w2] - nii\n",
    "    # number of times w1 and w2 do not appear together\n",
    "    noo = len(all_bigrams) - nii - noi - nio\n",
    "\n",
    "    O = [[nii, nio], [noi, noo]]\n",
    "\n",
    "    R1 = nii + nio\n",
    "    C1 = nii + noi\n",
    "    R2 = noi + noo\n",
    "    C2 = nio + noo\n",
    "    N = R1 + R2 + C1 + C2\n",
    "\n",
    "    E = [[R1*C1/N, R1*C2/N],[R2*C1/N, R2*C2/N]] \n",
    "\n",
    "    X_sq = sum((O[i][j] - E[i][j]) ** 2 / E[i][j] for i in range(len(O)) for j in range(len(O[i])))\n",
    "    p_val = stats.chi2.sf(X_sq, df=1)\n",
    "\n",
    "    data[(w1, w2)] = O, E, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_movies_filtered['tokens'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocations = {}\n",
    "\n",
    "for w1, w2 in tqdm(all_unique_bigrams):\n",
    "    if bigrams_counter[(w1, w2)] > 10 and data[(w1, w2)][2] < 0.001:\n",
    "        collocations.update({(w1, w2): bigrams_counter[(w1, w2)]})\n",
    "\n",
    "print(f\"\\nNumber of collocations: {len(collocations)}\")\n",
    "# Print out the top 20 of them by number of occurrences\n",
    "sorted(collocations.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recomputing the tokens with collocations\n",
    "df_movies_filtered['tokens'] = df_movies_filtered['abstract'].apply(lambda x: tokenize_and_clean_text3(x, collocations, \n",
    "                                                                                                       with_collocations=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the dataframe as a csv file called df_movies_tokens\n",
    "# df_movies_filtered.to_csv('data/df_movies_tokens.csv', index=False)\n",
    "\n",
    "df_movies_tokens = pd.read_csv('data/df_movies_tokens.csv', converters={'tokens': ast.literal_eval, 'genres': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the best partition using the Louvain method\n",
    "partition = community_louvain.best_partition(G, resolution=3)\n",
    "\n",
    "# Compute the modularity of this partition\n",
    "modularity = community_louvain.modularity(partition, G)\n",
    "\n",
    "print(\"Modularity found by Louvain algorithm:\", modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of communities and their sizes\n",
    "communities = {}\n",
    "for node, community in partition.items():\n",
    "    if community not in communities:\n",
    "        communities[community] = []\n",
    "    communities[community].append(node)\n",
    "\n",
    "num_communities = len(communities)\n",
    "community_sizes = sorted([len(nodes) for nodes in communities.values()], reverse=True)\n",
    "\n",
    "print(\"Number of communities:\", num_communities)\n",
    "print(\"Sizes of communities:\", community_sizes)\n",
    "# Check if the modularity is significantly different than 0\n",
    "if abs(modularity) > 0.01:  # or any other threshold you consider significant\n",
    "    print(\"The modularity is significantly different than 0.\")\n",
    "else:\n",
    "    print(\"The modularity is not significantly different than 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the partition and degree information\n",
    "df_communities = pd.DataFrame({\n",
    "    'actor': list(G.nodes),\n",
    "    'community': [partition[node] for node in G.nodes],\n",
    "    'degree': [G.degree(node) for node in G.nodes]\n",
    "})\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df_communities.to_csv('data/communities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the df_communities csv-file\n",
    "df_communities = pd.read_csv('data/communities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge actors_works_df with communities_df\n",
    "df = pd.merge(df_actors_filtered, df_communities, on='actor')\n",
    "df = df.drop(columns=[\"movies\"]).explode('movie_IDs')\n",
    "\n",
    "\n",
    "# Merge with abstracts_df\n",
    "df = pd.merge(df, df_movies_tokens, left_on='movie_IDs', right_on='movie_ID')\n",
    "\n",
    "# # Get abstract tokens for all communities\n",
    "all_communities_abstracts = df.groupby('community')['tokens'].agg(\"sum\").reset_index()\n",
    "\n",
    "all_communities_abstracts.columns = ['Community', 'Abstract Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_actors_filtered with communities df\n",
    "df_actors_communities = pd.merge(df_actors_filtered, df_communities, on='actor')\n",
    "\n",
    "# For community in top20_communities, calculate community attributes\n",
    "# Extract top 20 communities\n",
    "top20_communities_idx = df_communities['community'].value_counts().nlargest(45).index\n",
    "top20_community_actors = df_actors_communities[df_actors_communities['community'].isin(top20_communities_idx)]\n",
    "\n",
    "# Calculate in each community (group_by)\n",
    "def average_of_list(x):\n",
    "    return np.mean([val for sublist in x for val in sublist])\n",
    "def sum_dicts(dicts):\n",
    "    result = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k in result:\n",
    "                result[k] += v\n",
    "            else:\n",
    "                result[k] = v\n",
    "    return result\n",
    "top20_community_attributes = top20_community_actors.groupby('community').agg({\n",
    "    'actor': 'count',\n",
    "    'age': 'mean',\n",
    "    'ratings': [('mean', average_of_list)],\n",
    "    'popularity': 'mean',\n",
    "    'degree': [('mean', 'mean')],\n",
    "    'birthplace': lambda x: x.mode().iloc[0] if not x.mode().empty else None,\n",
    "})\n",
    "\n",
    "# Clean columns\n",
    "top20_community_attributes.columns = ['_'.join(col).strip() for col in top20_community_attributes.columns.values]\n",
    "top20_community_attributes.reset_index(inplace=True)\n",
    "\n",
    "# Calculate main genre in each community\n",
    "df_grouped = df_actors_communities.groupby('community')['genres'].apply(sum_dicts).reset_index()\n",
    "df_grouped = df_grouped.dropna()\n",
    "df_grouped = df_grouped.loc[df_grouped.groupby('community')['genres'].idxmax()]\n",
    "top20_community_attributes = pd.merge(top20_community_attributes, df_grouped, on='community', how='inner')\n",
    "top20_community_attributes = top20_community_attributes.drop(columns=['genres'])\n",
    "\n",
    "# Rename columns\n",
    "top20_community_attributes = top20_community_attributes.rename(columns={'birthplace_<lambda>': 'birthplace'})\n",
    "top20_community_attributes = top20_community_attributes.rename(columns={'level_1': 'main_genre'})\n",
    "\n",
    "# Create a success column that sums popularity and rating means\n",
    "rating_min = top20_community_attributes['ratings_mean'].min()\n",
    "rating_max = top20_community_attributes['ratings_mean'].max()\n",
    "popularity_min = top20_community_attributes['popularity_mean'].min()\n",
    "popularity_max = top20_community_attributes['popularity_mean'].max()\n",
    "\n",
    "rating_standardized = ((top20_community_attributes['ratings_mean'] - rating_min)/(rating_max - rating_min))\n",
    "popularity_standardized = ((top20_community_attributes['popularity_mean'] - popularity_min)/(popularity_max - popularity_min))\n",
    "\n",
    "top20_community_attributes['success'] = rating_standardized * popularity_standardized\n",
    "\n",
    "# top20_community_attributes.sort_values(by='ratings_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 9 communities and filter the abstracts for these communities\n",
    "# top9_communities = df_communities['community'].value_counts().nlargest(9).index\n",
    "top9_communities_idx = top20_community_attributes['ratings_mean'].nlargest(9).index\n",
    "\n",
    "top9_communities = top20_community_attributes.iloc[top9_communities_idx]['community']\n",
    "\n",
    "# find the top 9 community abstracts\n",
    "top9_communities_abstracts = all_communities_abstracts[all_communities_abstracts['Community'].isin(top9_communities)]\n",
    "\n",
    "top_terms = {}\n",
    "top_tfidf_terms = {}\n",
    "all_tfidf_terms = {}\n",
    "\n",
    "# (for less computational cost) get IDF for each term once and store the results\n",
    "idf_dict = {}\n",
    "for term in tqdm(set.union(*top9_communities_abstracts['Abstract Tokens'].apply(set))):\n",
    "    idf_dict[term] = math.log(len(top9_communities_abstracts) / \n",
    "                              sum(term in abstract for abstract in top9_communities_abstracts['Abstract Tokens']))\n",
    "\n",
    "\n",
    "for community in tqdm(top9_communities):\n",
    "    # Get abstract tokens for the community\n",
    "    abstract_tokens = top9_communities_abstracts[top9_communities_abstracts['Community'] == community]['Abstract Tokens'].values[0]\n",
    "    \n",
    "    # The top 10 TF terms\n",
    "    term_counts = Counter(abstract_tokens)\n",
    "    top_terms[community] = term_counts.most_common(10)\n",
    "\n",
    "\n",
    "    tfidf = {}\n",
    "    for term, count in term_counts.items():\n",
    "        # TF for all terms in top 9 communities\n",
    "        tf = count / len(abstract_tokens)\n",
    "\n",
    "        # Get IDF from the precalculated dictionary\n",
    "        idf = idf_dict[term]\n",
    "        \n",
    "        # TF-IDF for all terms in top 9 communities\n",
    "        tfidf[term] = tf * idf\n",
    "\n",
    "    all_tfidf_terms[community] = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top 10 TF-IDF words\n",
    "    top_tfidf_terms[community] = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_tfidf_terms.items():\n",
    "    print(f\"Community {item[0]}:\")\n",
    "    print(', '.join([term[0] for term in item[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top9_communities_df = df_communities[df_communities['community'].isin(top9_communities)]\n",
    "\n",
    "# Group by Community and Actor, and sum the Degree to get the total Degree for each Actor in each Community\n",
    "grouped_df = top9_communities_df.groupby(['community', 'actor'])['degree'].sum().reset_index()\n",
    "\n",
    "# Make new dataframe storing the top 5 actors by degree for each community\n",
    "top_actors = grouped_df.groupby('community').apply(lambda x: x.nlargest(5, 'degree')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all_tfidf_terms as a JSON file in the data folder\n",
    "with open('data/all_tfidf_terms.json', 'w') as f:\n",
    "    json.dump(all_tfidf_terms, f)\n",
    "\n",
    "# Save top9_communities as a CSV file in the data folder\n",
    "pd.DataFrame(top9_communities, columns=['community']).to_csv('data/top9_communities.csv', index=False)\n",
    "\n",
    "# Save top_actors as a CSV file in the data folder\n",
    "top_actors.to_csv('data/top_actors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files for wordcloud\n",
    "top_actors = pd.read_csv('data/top_actors.csv')\n",
    "\n",
    "top9_communities = pd.read_csv('data/top9_communities.csv')\n",
    "\n",
    "with open('data/all_tfidf_terms.json', 'r') as file:\n",
    "    all_tfidf_terms = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to closer investigate the communities, the TF-IDF wordcloud plots have been made for the 9 best rated communities. This could help explain what kind of attributes the communities form around - if they form around similar movie types. The wordclouds are based on the TF-IDF, meaning that the sizes of the words on the plots are dependent on the corresponding TF-IDF values. This is done to obtain descriptive and defining words to make it easier to characterize and differentiate the communities. In addition to the wordclouds, the top 5 highest degree actors from each community are printed, since these are the most central actors in the community and could help at understanding the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(24, 24))\n",
    "\n",
    "for i, community in enumerate(top9_communities[\"community\"]):\n",
    "    # Get the already calculated top TF-IDF terms for the community\n",
    "    tfidf_terms = dict(all_tfidf_terms[str(community)])\n",
    "    \n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = None, \n",
    "                min_font_size = 10).generate_from_frequencies(tfidf_terms)\n",
    "    \n",
    "    # Subplot indices\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    # Plot the word cloud\n",
    "    axs[row, col].imshow(wordcloud)\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "    # Get the names of the top three actors in the community\n",
    "    top_three_actors = top_actors[top_actors['community'] == community]['actor'].values\n",
    "    # Create the title string with newlines\n",
    "    title_string = \"\\n\".join(top_three_actors)\n",
    "\n",
    "    # Set title of subplot to the names of the top three actors, each on a new line\n",
    "    axs[row, col].set_title(f\"Top actors in community {community}:\\n{title_string}\", fontsize=15)\n",
    "\n",
    "# Remove any extra subplots\n",
    "for j in range(i+1, 9):\n",
    "    fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "plt.tight_layout(pad = 1) \n",
    "plt.subplots_adjust(hspace = 0.25)  # Increase the hspace value\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word cloud to png\n",
    "# fig.savefig('wordcloud.png')\n",
    "\n",
    "# load wordcloud\n",
    "Image(filename='figures/wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen on the plot above, the resulting 9 wordcloud plots are consisting of very different words with different sizes, that are easy to distinct. It is very clear that the communties in these cases are based on types of movies. \n",
    "All the 9 communties are easily identifiable, but to highlight a few, the 3rd community in the bottom row is clearly dominated by the Lord of the Rings movies, seen by the words \"dwarv\" and \"hobbit\" and the names \"bilbo\", \"gandalf\", \"smaug\", \"erebor\", etc.\n",
    "Another interesting observation are the collocations in the first community in the 2nd row, which is a Marvel Avengers (superhero) community. Here, many of the character names are 2-worded, which makes those collocations central in terms of its TF-IDF, since they are central in this community and very rarely seen in other communities. These collocations are for example \"spider_man\", \"iron_man\", \"captain_america\" and \"stan_lee\".\n",
    "In both the mentioned communities (and in the other, too), the printed highest degree actors are all very central characters in the movies described by the wordclouds, which makes sense sinse they work with most of the other actors and must star in most of the movies in the community.\n",
    "\n",
    "The rest of the communities, starting from the top left, could be categorized as anime (japanese animation), wrestling, korean, Harry Potter, japanese, cartoon and Star Wars.\n",
    "To further visualize the wordclouds and get a deeper understanding of the communities, a movie poster has been generated based on the top 10 TF-IDF words in 3 chosen communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HTML string for images\n",
    "html_str = \"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: flex-start;\">\n",
    "    <div>\n",
    "        <img src='figures/marvel.png' style='width: 250px;'>\n",
    "        <p style=\"text-align: center;\">Community 8</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='figures/anime_dj.png' style='width: 250px;'>\n",
    "        <p style=\"text-align: center;\">Community 43</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='figures/harry_potter.png' style='width: 250px;'>\n",
    "        <p style=\"text-align: center;\">Community 57</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='figures/starwars.png' style='width: 250px;'>\n",
    "        <p style=\"text-align: center;\">Community 19</p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the images\n",
    "display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the wordclouds and movie posters, it can be concluded that very distinct communities form inside the actor collaboration network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What distinguishes well performing communities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step in the search to the research question, general attributes for each community are compared. This way, it is investigated if there is something that sets the well-performing communties apart from the less successful communities.\n",
    "\n",
    "Here, only the communities of a size of 10 actors or more are considered, so the success of a community of eg. 2 actors isn't considered. This filtering results in 45 communities, containing 3348 of the actors (5100 before community size filtering).\n",
    "\n",
    "For each community, the following attributes are calculated:\n",
    "* Community age: Mean actor age\n",
    "* Community genre: most common genre of all movies made by community actors. Like community rating, movies starring eg. 2 actors from the community are counted twice.\n",
    "* Community birthplace: Most common actor birthplace\n",
    "* Community rating: Mean of all ratings obtained by all the actors. This way, a movie starring eg. 3 of the actors in the community is weighted higher than a movie only starring one actor.\n",
    "* Community popularity: Mean actor popularity\n",
    "\n",
    "Then, each community is plotted with their attribute variables (age/genre/birthplace) vs the goal metric (rating/popularity) in scatter or Mean-SD plots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize=(16, 18))\n",
    "\n",
    "metrics = ['ratings_mean', 'popularity_mean', 'success']\n",
    "metrics_map = {'ratings_mean': 'community rating', 'popularity_mean': 'community popularity', 'success': 'community success'}\n",
    "x_values = ['age_mean', 'main_genre', 'birthplace']\n",
    "titles = ['Community Age vs ', 'Community Genre vs ', 'Community birthplace vs ']\n",
    "colors = ['blue', 'green', 'red']  # List of colors for each row\n",
    "\n",
    "for i, metr in enumerate(metrics):\n",
    "    for j, x_val in enumerate(x_values):\n",
    "        ax[i,j].set_title(titles[j] + metrics_map[metr])\n",
    "        \n",
    "        # Use scatter plots for 'age_mean' and point plots for the rest\n",
    "        if x_val == 'age_mean':\n",
    "            sns.regplot(x=top20_community_attributes[x_val], y=top20_community_attributes[metr], color=colors[i], ax=ax[i,j], ci=95)\n",
    "            ax[i,j].set_xlabel('Mean Age')\n",
    "            ax[i,j].set_ylabel(metrics_map[metr])\n",
    "            \n",
    "            # Calculate Spearman's correlation\n",
    "            corr, _ = stats.spearmanr(top20_community_attributes[x_val], top20_community_attributes[metr])\n",
    "            ax[i,j].text(0.1, 0.9, f'Spearman: {corr:.2f}', transform=ax[i,j].transAxes)\n",
    "        else:\n",
    "            order = top20_community_attributes.groupby(x_val)[metr].mean().sort_values().index\n",
    "            sns.pointplot(x=x_val, y=metr, data=top20_community_attributes, order=order, capsize=.1, linestyle='none', ax=ax[i,j], color=colors[i])\n",
    "            ax[i,j].set_ylabel(metrics_map[metr])\n",
    "\n",
    "        # Rotate x-axis labels for each subplot\n",
    "        for label in ax[i,j].get_xticklabels():\n",
    "            label.set_rotation(60)\n",
    "        \n",
    "        # Adjust age ticks for the first column of subplots\n",
    "        if j == 0:\n",
    "            start, end = ax[i,j].get_xlim()\n",
    "            start, end = int(round(start)), int(round(end))\n",
    "            ax[i,j].xaxis.set_ticks(np.arange(start, end, 5))\n",
    "        \n",
    "        # Add grid to all plots\n",
    "        ax[i,j].grid(True)\n",
    "\n",
    "# add vertical space\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure to a png file\n",
    "# fig.savefig('figures/community_metrics.png')\n",
    "\n",
    "# Load community metrics plot\n",
    "Image(filename='figures/community_metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 2 rows of the plot above, the community attributes are plotted vs the rating and popularity. Here, it is seen that age has no significant impact on success, as the horizontal line is within the confidence interval. Also, the (Spearman's) correlation is very low. Both genre and birthplace seems to have an effect on performance. (Note that no confidence interval is shown when there is only one datapoint for a category).\n",
    "Despite a very limited sample size, Japan and Korea performs (significantly, for Japan) better than many of the other countries in rating, while US does the same in popularity.\n",
    "\n",
    "However, these two metrics do not encapsulate the success of a community, or an actor, for that matter. For example, Japan has the highest mean rating but lowest mean popularity, which means that they are extremely liked but only for a small audience. \n",
    "\n",
    "Because of these result differences in popularity and rating, a new target measure is made: \"success\". This is made by standardizing rating and popularity and multiplying them. This way, a community only has a high success-score if it has both a high rating and popularity. The popularity score is plotted in the 3rd row in the plot above.\n",
    "\n",
    "Again, age has no significant impact on success. And because each community's success is now weighted by multiple factors, countries like Japan and US are performing less confidently in rating and popularity, respectively. Now, no single country or genre seems to be performing far better than the others, but there are still some countries (Japan, UK, Korea, US and Australia) that seems to perform better than others. \n",
    "\n",
    "\n",
    "**To summarize the result**, when looking at rating or popularity in isolation, some countries and genres seem to perform significantly better than many of the others. So, if the goal is to perform at one of these measures (i.e if you want to be very famous or highly rated), the plan would be to move to Japan and make animated movies.\n",
    "However, if you want to be \"successful\" (i.e. be both popular and highly rated), there is no plan that \n",
    "leads to a significantly more successful acting career."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "* What went well?\n",
    "\n",
    "nothing...\n",
    "\n",
    "* What is still missing? What could be improved? Why?\n",
    "\n",
    "The data collected about the most popular actors from TMDB reveals notable biases which should be considered when interpreting the results of this project. It was observed that there was a significant imbalance in the actor nationalities, as over 50% of the actors in our network originated from the United States, while the second most represented nationality is the United Kingdom which only accounted for 13% of the total actors. This is showing a bias towards American actors for the most popular actors within TMDB. This bias in the nationality representation among actors could be due to various reasons, such as the geographical focus of TMDB’s user base, the language dominance of English in the film industry, and the popularity of Hollywood productions on the platform. However it is important to address that this bias may skew analyses and conclusions drawn from our data, especially concerning those related to nationality or culture. To improve the diversity of our dataset, and improve the reliability of our results and conclusions, the data could be combined with information from other data sources, that are not as centered around the American film industry and userbase regarding popular actors. \n",
    "\n",
    "Another bias observed in our data, is in the distribution of main genres among actors. However this one is more questionable in the sense that it is a homemade attribute based on the genres of the movies the actors in our network have participated in after 2010. But it was observed that the Comedy genre was representing 35% of the actors’ main genre in movies they acted in while other genres had a much lower representation percentage. This indicates a bias towards actors association with comedy in TMDB but might just reflect the user preferences or industry trends. However, it would enhance the fairness of our analysis if the genres were more evenly distributed, as some of the lower represented genres may not be well portrayed. \n",
    "Therefore to improve our data for future work and mitigate such biases, the data collection should again focus on diversifying data sources or using sampling strategies that account for demographic and cultural variations.\n",
    "\n",
    "For future work, it could be interesting to look further into the attributes and how they contribute to success in terms of rating and popularity. However, Korean love stories might be considered generally good where American love stories might be less successful. The different combinations of attributes is something that this project has not explored but could be an idea for future work. This could be done e.g. through heatmaps, where the color illustrates success, and the attributes on the axis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
