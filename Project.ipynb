{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Assignment B\n",
    "Link to git repository: https://github.com/ongiboy/computational_social_science\n",
    "\n",
    "Group members:\n",
    "* Christian Ong Hansen (s204109)\n",
    "* Kavus Latifi Yaghin (s214601)\n",
    "* Daniel Damkjær Ries (s214641)\n",
    "\n",
    "Group member's contribution:\n",
    "* Every task was made in collaboration by all members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is your dataset\n",
    "\n",
    "Our dataset originates from The Movie Database (TMDB) via its API. The data is collected from the \"Popular People\" tab on the webpage, and includes the top actors from the first 250 pages, with each page containing data for 20 actors. The popularity score attributed to actors by TMDB is a metric not publicly disclosed, but it's generally understood to consider factors like page views, favorites, watchlists, and recent activity.\n",
    "\n",
    "The data is structured into two main dataframes: one for actors and their attributes (name, ID, gender, age, birthplace, and filmography), and another for movies featuring these actors (rating, popularity, genres, release date, and abstract for text analysis). This setup forms the basis for our actor collaboration network and analysis, which is essential for answering our research question: Based on collaborations among the most popular actors, do distinct communities form, and if so, what characterizes the most succesful communities?\n",
    "\n",
    "* Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "The reason for us choosing The Movie Database (TMDB) as the data source for our project is because of our interest in exploring and analyzing the film industry by an actor collaboration network. TMDB has a broad coverage of the film industry and it has built a reputation for being a well-known data source within the field. TMDB is widely recognized for its movie-related data, containing details on actors, movies, genres, ratings, popularity etc.. The rate limit for the API (50 calls per second) was also an attractive factor. \n",
    "\n",
    "It was therefore believed that through TMDB's API, it would be possible to access up-to-date and reliable data on popular actors and their participation in movies, enabling us to explore actor interactions and movie trends effectively. In this way the TMDB datasource can be used to contribute valuable insights to the broader understanding of the film industry landscape.\n",
    "\n",
    "* What was your goal for the end user’s experience?\n",
    "\n",
    "The goal of this project for the end user's experience was to provide insights into the characteristics and dynamics of successful actor communities. By analyzing data from popular actors and their collaborations in a network, we sought to offer a user-friendly interface for exploring trends, identifying influential factors, and gaining a deeper understanding of what drives success in the film industry in a an actor perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats\n",
    "* Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "Using the TMDB API it was possible to retrieve information about popular actors just by using an API-key and selecting the desired number of pages with 20 actors in each page. 250 pages were chosen as a reasonable amount, which in theory would lead to a raw dataset consisting of 6000 actors, but in practice ended up being 4389 actors. This would be the first dataframe \"df_actors\". Since it wasn't possible to use filters directly in the API-call, data processing was performed after the data was collected. The information about these actors that was collected were the attributes: name, id, gender, birthday, place_of_birth. \n",
    "\n",
    "Initially, all rows with missing values on the attributes of interest were removed and duplicate rows were removed. Since all the collected information about the actors is crucial for the analysis, this seemed a reasonable part of the cleaning. After this short processing, the actor dataframe would consist of 4048 actors. \n",
    "\n",
    "Using the retrieved actor IDs, it was now possible to get a complete list of the movies that each actor has played a role in as well as the movie ID - again using the TMDB API. This also laid the foundation for the second dataframe \"df_movies\" with the purpose of being our own little movie-database containing information about all the movies that the actors in the \"df_actors\" dataframe have been part of. Before any processing, the dataframe included 67587 movies.\n",
    "\n",
    "With yet another API call using the found movie IDs, the desired information could be obtained, which consisted of the attributes: rating, popularity, genres, release date and movie abstract. After retrieval of movie information in the movies dataframe, all rows with missing values in any of the attributes were removed. Furthermore, it was decided for this project to only focus on \"recent\" movies, which in this case is defined as movies from 2010 and until the date of collection (08-05-2024). After this processing the movie dataset consisted of 26494 movies. This was also done to avoid having a too dense network later.\n",
    "\n",
    "The outfiltered movies in the movies dataframe, were also removed from the actor dataframe, so the two dataframes would be consistent. \n",
    "\n",
    "Through further investigation of the actors dataframe, it was found that the \"birthplace\" column needed processing. Firstly, the birthplace was reduced to only include the country of birth for the actors, but it was also found that the same countries were spelled differently. The country could be in different languages, symbols, etc., which could introduce misleading findings. The birthplace column was normalized so there would be no double instances of a country of birth. \n",
    "\n",
    "For the movies dataframe, the movie popularity seemed to have outliers. It was found that around 99% of the moves had a popularity measure under 100 but a few values going all the way up to over 2000. This led us to setting a cap at 100.\n",
    "\n",
    "It was in our interest to select a main genre for each actor. Each movie has a list of given genres, and by finding the genre that appeared the most in the movies that the actor has been part of, a main genre for an actor was found. Since about 50% of all movies had the \"Drama\" genre, this was only counted if the movie had two or less genres in its genre list to avoid that all actors would be \"Drama\" actors. We also found that the \"Drama\" genre was generally not very descriptive of the movie when there were more than 2 genres.\n",
    "\n",
    "Finally, from the TMDB API there is no direct actor rating measure, which gave us the idea to create our own measure. This measure was created as the mean of the ratings from all the movies the actor has acted in, of the more recent movies (2010-2024). \n",
    "\n",
    "Now that the data has been cleaned and preprocessed, it was time to create the actual network. The edges of the network were weighted by the number of times two actors had collaborated. This led to a very dense network, and therefore a threshold was set, so that there would only be an edge if the actors had collaborated at least twice. The final network consisted of the 4254 actors/nodes with 23399 edges. \n",
    "\n",
    "* Write a short section that discusses the dataset stats\n",
    "\n",
    "Our main dataset is as mentioned before, two dataframes, one for actor information and the other for movie information.\n",
    "These datasets have the following stats:\n",
    "\n",
    "Actor dataframe: 4048 rows of size 4.306 KB\n",
    "\n",
    "Movie Dataframe: 23399 rows of size 9.569 KB\n",
    "\n",
    "From these a network was made of actor collaborations, with the following stats:\n",
    "4048 nodes and 15116 edges, of size 3.177 KB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval and cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "import json\n",
    "from threading import Lock\n",
    "import matplotlib.cm as cm\n",
    "import pycountry\n",
    "import re\n",
    "import netwulf as nw\n",
    "import matplotlib.patches as mpatches\n",
    "import community.community_louvain as community_louvain\n",
    "from IPython.display import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "api_key = \"f5813332cb558d374cbcb057ea2fc48b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to make API calls (**ADD ALL AND RENAME**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "lock = Lock()\n",
    "\n",
    "def movie_title_and_IDs_from_actor_ID(actor_id, session):\n",
    "    global counter\n",
    "    url = f\"https://api.themoviedb.org/3/person/{actor_id}/movie_credits?api_key={api_key}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    with lock:\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Processed {counter} actors\")\n",
    "    # Return the whole movie dictionary, not just the title\n",
    "    return [movie['title'] for movie in data['cast']], [movie['id'] for movie in data['cast']]\n",
    "\n",
    "def actor_info_from_page(page, session):\n",
    "    url = f\"https://api.themoviedb.org/3/person/popular?api_key={api_key}&page={page}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "    people = []\n",
    "    for person in data['results']:\n",
    "        person_url = f\"https://api.themoviedb.org/3/person/{person['id']}?api_key={api_key}\"\n",
    "        person_response = session.get(person_url)\n",
    "        person_data = person_response.json()\n",
    "        people.append((person['name'], person['id'], person_data['gender'], person_data['birthday'], person_data['place_of_birth'], person_data['popularity']))\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "lock = Lock()\n",
    "\n",
    "def fetch(session, url):\n",
    "    global counter\n",
    "    future = session.get(url, headers=headers)\n",
    "    with lock:\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print(f\"Processed {counter} movies\")\n",
    "    \n",
    "    return future\n",
    "\n",
    "def movie_info_from_movie_ID(urls):\n",
    "    with FuturesSession() as session:\n",
    "        futures = [fetch(session, url) for url in urls]\n",
    "        responses = [future.result().json() for future in tqdm(futures, total=len(futures))]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve actor information (name, id, gender, birthday, birthplace, popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        pages = list(range(1, 301))\n",
    "        fetch_page_with_session = partial(actor_info_from_page, session=session)\n",
    "        people = list(executor.map(fetch_page_with_session, pages))\n",
    "\n",
    "actor_names, actor_ids, actor_genders, actor_birthdays, actor_birthplaces, actor_popularities = zip(*itertools.chain(*people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Actor Dataframe and initial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with 'actors' column\n",
    "df_actors = pd.DataFrame(actor_names, columns=['actor'])\n",
    "\n",
    "# Add 'ids', 'genders', and 'birthplaces' columns to the DataFrame\n",
    "df_actors['actor_id'] = actor_ids\n",
    "df_actors['gender'] = actor_genders\n",
    "df_actors['age'] = actor_birthdays # This is not the age, but the birthday\n",
    "df_actors['birthplace'] = actor_birthplaces\n",
    "df_actors['popularity'] = actor_popularities\n",
    "\n",
    "# Change birthplaces so that it only contains the country (text after the last comma)\n",
    "df_actors['birthplace'] = df_actors['birthplace'].str.split(',').str[-1]\n",
    "\n",
    "# Change birthday to age\n",
    "df_actors['age'] = pd.to_datetime(df_actors['age'], errors='coerce')\n",
    "df_actors['age'] = (pd.to_datetime('today') - df_actors['age']).dt.days // 365\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_actors.dropna(inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df_actors.drop_duplicates(subset='actor_id', inplace=True)\n",
    "#reset index\n",
    "df_actors.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve movie titles and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch movies for each actor and add them to 'movies' and 'movie_IDs' columns\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    movie_titles_and_ids_from_actor_ID_with_session = partial(movie_title_and_IDs_from_actor_ID, session=session)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        movies_and_ids = list(executor.map(movie_titles_and_ids_from_actor_ID_with_session, df_actors['actor_id']))\n",
    "\n",
    "# Add 'movies' column to df_actors\n",
    "df_actors['movies'] = [x[0] for x in movies_and_ids]\n",
    "df_actors['movie_IDs'] = [x[1] for x in movies_and_ids]\n",
    "\n",
    "# Flatten movies_and_ids into two separate lists\n",
    "movies = [item for sublist in [x[0] for x in movies_and_ids] for item in sublist]\n",
    "ids = [item for sublist in [x[1] for x in movies_and_ids] for item in sublist]\n",
    "\n",
    "# Convert the lists into a list of dictionaries\n",
    "movies_and_ids_dict = [{'movie': movie, 'movie_ID': id} for movie, id in zip(movies, ids)]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_movies = pd.DataFrame(movies_and_ids_dict)\n",
    "df_movies = df_movies.drop_duplicates(subset='movie_ID')\n",
    "df_movies.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect movie information trough API (rating, popularity, genre, release date, abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the URLs\n",
    "urls = [f\"https://api.themoviedb.org/3/movie/{id}?api_key={api_key}\" for id in df_movies[\"movie_ID\"]]\n",
    "print('urls are prepared')\n",
    "\n",
    "# Fetch all responses\n",
    "responses = movie_info_from_movie_ID(urls)\n",
    "print('all responses are fetched')\n",
    "\n",
    "# Process the responses\n",
    "for i, response in enumerate(responses):\n",
    "    # if i % 100 == 0:\n",
    "    #     print(f\"Processing response {i+1}/{len(responses)}\")\n",
    "    if isinstance(response, Exception):\n",
    "        print(f\"Error: {response}\")\n",
    "        continue  # Skip this response\n",
    "    # Process the response here\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "ratings = []\n",
    "popularities = []\n",
    "genres = []\n",
    "release_dates = []\n",
    "abstracts = []\n",
    "\n",
    "# Process the responses one by one\n",
    "for data in tqdm(responses):\n",
    "    ratings.append(data.get('vote_average'))\n",
    "    popularities.append(data.get('popularity'))\n",
    "    genres.append([genre['name'] for genre in data.get('genres', [])])\n",
    "    release_dates.append(data.get('release_date'))\n",
    "    abstracts.append(data.get('overview'))\n",
    "\n",
    "# Assign the lists to the DataFrame columns\n",
    "df_movies['rating'] = ratings\n",
    "df_movies['popularity'] = popularities\n",
    "df_movies['genres'] = genres\n",
    "df_movies['release_date'] = release_dates\n",
    "df_movies['abstract'] = abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial preprocessing of Movies Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_movies.drop_duplicates(subset='movie', inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_movies.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with empty lists in 'genres' column\n",
    "df_movies = df_movies[df_movies['genres'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Remove rows with empty release dates\n",
    "df_movies = df_movies[df_movies['release_date'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if abstract is missing\n",
    "df_movies = df_movies[df_movies['abstract'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if rating is missing\n",
    "df_movies = df_movies[df_movies['rating'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Drop row if popularity is missing\n",
    "df_movies = df_movies[df_movies['popularity'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Reset index\n",
    "df_movies.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove old and future movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes so that each actor is associated with the movies they have acted in\n",
    "df_actors_filtered = df_actors.copy()\n",
    "df_movies_filtered = df_movies.copy()\n",
    "df_movies_filtered.rename(columns={'movie_ID': 'movie_IDs'}, inplace=True)\n",
    "df_movies_filtered.rename(columns={'popularity': 'movie_popularity'}, inplace=True)\n",
    "df_actors_filtered.drop(columns=['movies'], inplace=True)\n",
    "\n",
    "df_actors_filtered = df_actors_filtered.explode('movie_IDs').reset_index(drop=True)\n",
    "\n",
    "df_actors_movies = df_actors_filtered.merge(df_movies_filtered, on='movie_IDs', how='inner')\n",
    "\n",
    "# Remove all rows where the release date is before 2010 or after 2024 (to avoid movies that have not been released yet)\n",
    "df_actors_movies = df_actors_movies[(df_actors_movies['release_date'] >= '2010-01-01') & (df_actors_movies['release_date'] <= '2024-05-08')]\n",
    "\n",
    "# Collapse the the actors in the actor column so there is only one row per actor and the movies and movie_IDs are stored in lists\n",
    "df_actors_filtered = df_actors_movies.groupby('actor').agg({'actor_id': 'first',\n",
    "                                                             'gender': 'first',\n",
    "                                                             'birthplace': 'first',\n",
    "                                                             'age': 'first',\n",
    "                                                             'popularity': 'first',\n",
    "                                                             'movie': list, \n",
    "                                                             'movie_IDs': list}).reset_index()\n",
    "\n",
    "df_movies_filtered = df_actors_movies.drop_duplicates(subset=['movie']).groupby('movie').agg({'movie_IDs': 'first',\n",
    "                                                                                               'rating': 'first',\n",
    "                                                                                               'popularity': 'first',\n",
    "                                                                                               'genres': 'first',\n",
    "                                                                                               'release_date': 'first',\n",
    "                                                                                               'abstract': 'first'}).reset_index()\n",
    "\n",
    "df_actors_filtered.rename(columns={'movie': 'movies'}, inplace=True)\n",
    "df_movies_filtered.rename(columns={'movie_IDs': 'movie_ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the birthplace column, to avoid having the same country spelled differently and removing symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean birthplace column in actor dataframe\n",
    "len_bef_clean = len(df_actors_filtered['birthplace'].unique())\n",
    "\n",
    "country_names = [country.name for country in pycountry.countries]\n",
    "\n",
    "def normalize_country_name(name):\n",
    "    # strip name\n",
    "    name = name.strip()\n",
    "    # replace '.' with ''\n",
    "    name = name.replace('.', '').replace(']', '')\n",
    "\n",
    "    if \"UK\" in name or \"İngiltere\" in name:\n",
    "        return \"United Kingdom\"\n",
    "    try:\n",
    "        # Try to get the country object\n",
    "        country = pycountry.countries.get(name=name)\n",
    "        if country is not None:\n",
    "            # If the country object is found, return the official name\n",
    "            return country.name\n",
    "        else:\n",
    "            # If the country object is not found, try to find it by its common name\n",
    "            country = pycountry.countries.search_fuzzy(name)\n",
    "            return country[0].name\n",
    "    except LookupError:\n",
    "        # Standardizing names\n",
    "        for country_name in country_names:\n",
    "            if country_name in name:\n",
    "                name = country_name\n",
    "\n",
    "        # Fixing abbreviations and wird instances\n",
    "        if \"Russia\" in name:\n",
    "            return \"Russian Federation\"\n",
    "        elif \"Türkiye\" in name or \"Turkey\" in name:\n",
    "            return \"Türkiye\"\n",
    "        elif \"USA\" in name or \" US\" in name or \"United States\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Korea\" in name:\n",
    "            return \"Korea, Republic of\"\n",
    "        elif \"Czech\" in name:\n",
    "            return \"Czechia\"\n",
    "        # Hardcoded, could use package to translate, maybe not necessary, few occurences\n",
    "        elif \"TX\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Frankrike\" in name:\n",
    "            return \"France\"\n",
    "        elif \"Afrique du Sud\" in name:\n",
    "            return \"South Africa\"\n",
    "        elif \"Irlanda\" in name:\n",
    "            return \"Ireland\"\n",
    "        elif \"中国\" in name or \"中华民国\" in name or \"重庆\" in name or \"南京\" in name:\n",
    "            return \"China\"\n",
    "        \n",
    "        \n",
    "        # Updating from old names\n",
    "        if \"now\" in name:\n",
    "            match = re.search(r'\\[now (.*?)', name)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        # If the country is not found, return the original name\n",
    "        return name\n",
    "\n",
    "# Normalize the country names in the DataFrame\n",
    "df_actors_filtered['birthplace'] = df_actors_filtered['birthplace'].apply(normalize_country_name)\n",
    "\n",
    "len_aft_clean = len(df_actors_filtered['birthplace'].unique())\n",
    "print(\"length before cleaning\", len_bef_clean)\n",
    "print(\"length after cleaning\", len_aft_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram\n",
    "counts, bin_edges = np.histogram(df_actors_filtered['popularity'], bins=100)\n",
    "\n",
    "# Add a small constant to counts to avoid log(0)\n",
    "counts = counts + 1e-10\n",
    "\n",
    "# Plot the histogram\n",
    "plt.plot(bin_edges[:-1], counts)\n",
    "\n",
    "# Set the scale of both axes to logarithmic\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_actors_filtered['popularity'], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the age distribution, NOTHINGS DONE YET\n",
    "df_actors_filtered['age'].hist(bins=30, edgecolor='white')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of top 10 actor birthplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts of each unique value in the 'birthplace' column\n",
    "birthplace_counts = df_actors_filtered['birthplace'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each birthplace out of the total number of actors\n",
    "birthplace_percentages = (birthplace_counts / df_actors_filtered['birthplace'].count()) * 100\n",
    "\n",
    "# Get the top 5 birthplaces\n",
    "top_5_birthplaces = birthplace_percentages[:10]\n",
    "\n",
    "# Create a color map\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(top_5_birthplaces)))\n",
    "\n",
    "# Create a bar plot of the top 5 birthplaces with different colors for each bar\n",
    "top_5_birthplaces.plot(kind='bar', color=colors, edgecolor='black')\n",
    "\n",
    "plt.title('Top 10 Birthplaces of Actors')\n",
    "plt.xlabel('Birthplace')\n",
    "plt.ylabel('Percentage of Actors (%)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The number of unique birthplaces:\",len(df_actors_filtered['birthplace'].unique()))\n",
    "# Get the birthplaces with a count of 1\n",
    "birthplaces_with_count_1 = birthplace_counts[birthplace_counts == 1]\n",
    "\n",
    "print(\"Number of countries only associated with One actor in our dataset\",len(birthplaces_with_count_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: TMDB is very biased towards USA with their popular actors data that we have retrieved (over 50% of the total number of actors). Therefore most of the movies are american and the actors from different countries are biased towards the american movie industry.\n",
    "\n",
    "* It is also worth noting that 44 of the 106 unique birthplaces in our dataset is only associated with one actor, which is also a factor causing the unexpected low assortativity for the birthplace attribute. \n",
    "\n",
    "* To fix this or make it more representative, we could webscrape from other websites to make a less biased dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the main genre for the popular actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the genre counts and main genre of an actor\n",
    "def find_genres_and_main_genre(movies_list, remove_drama=False): # nearly half of all movies have drama as genre, so not descriptive\n",
    "    # Get the genres of the movies\n",
    "    genres = [genre for movie in movies_list for genre in movie_genres.get(movie, [])]\n",
    "    # Count the frequency of each genre\n",
    "    genre_counts = pd.Series(genres).value_counts()\n",
    "    # Remove the 'Drama' genre if specified\n",
    "    if remove_drama and len(genre_counts) > 2:\n",
    "        genre_counts.drop('Drama', errors='ignore', inplace=True)\n",
    "        #genre_counts.drop('Thriller', errors='ignore', inplace=True)\n",
    "    \n",
    "    # Return the genre counts as a dictionary and the main genre\n",
    "    return genre_counts.to_dict(), genre_counts.idxmax() if not genre_counts.empty else None\n",
    "\n",
    "movie_genres = df_movies_filtered.set_index('movie')['genres'].to_dict()\n",
    "# Apply the function to the 'movies' column of the actors DataFrame and create two new columns\n",
    "df_actors_filtered['genres'], df_actors_filtered['main_genre'] = zip(*df_actors_filtered['movies'].apply(find_genres_and_main_genre, remove_drama=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor popularity and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the average rating and popularity of an actor\n",
    "def find_actor_rating(movies_list):\n",
    "    # Get the ratings for each movie\n",
    "    ratings = [movie_ratings[movie] for movie in movies_list if movie in movie_ratings]\n",
    "    avg_rating = np.mean(ratings)\n",
    "\n",
    "    return ratings, avg_rating\n",
    "\n",
    "movie_ratings = df_movies_filtered.set_index('movie')['rating'].to_dict()\n",
    "\n",
    "# Apply the function to the 'movies' column of the actors DataFrame and create two new columns\n",
    "df_actors_filtered['ratings'], df_actors_filtered['avg_rating'] = \\\n",
    "        zip(*df_actors_filtered['movies'].apply(find_actor_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv files\n",
    "df_actors_filtered.to_csv('data/actors.csv', index=False)\n",
    "df_movies_filtered.to_csv('data/movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes from csv files\n",
    "df_actors_filtered = pd.read_csv('data/actors.csv', converters={'movies': ast.literal_eval, 'movie_IDs': ast.literal_eval, 'ratings': ast.literal_eval, 'genres': ast.literal_eval})\n",
    "df_movies_filtered = pd.read_csv('data/movies.csv', converters={'genres': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "movie_to_actors = defaultdict(list)\n",
    "\n",
    "for _ , row in df_actors_filtered.iterrows():\n",
    "    actor = row['actor']\n",
    "    movies = row['movies']\n",
    "    for movie in movies:\n",
    "        movie_to_actors[movie].append(actor)\n",
    "\n",
    "edges = defaultdict(int)\n",
    "for actors in movie_to_actors.values():\n",
    "    for pair in itertools.combinations(sorted(actors), 2):\n",
    "        edges[pair] += 1\n",
    "\n",
    "# Step 2: Create the Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "for _ , row in df_actors_filtered.iterrows():\n",
    "    G.add_node(row['actor'], gender=row['gender'], age=row['age'], birthplace=row['birthplace'], main_genre=row['main_genre'],\n",
    "               avg_rating=row['avg_rating'], popularity=row['popularity'])\n",
    "\n",
    "# Add edges with weights\n",
    "for edge, weight in edges.items():\n",
    "\n",
    "# Adding only the edges with weight >= 2 to get less dense graph\n",
    "    if weight >= 2:\n",
    "        G.add_edge(*edge, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of nodes and edges\n",
    "N = G.number_of_nodes()\n",
    "L = G.number_of_edges()\n",
    "\n",
    "print(\"Number of nodes: \", N)\n",
    "print(\"Number of edges: \", L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, theory and analysis\n",
    "\n",
    "* Describe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\n",
    "\n",
    "Degree analysis: \n",
    "\n",
    "In the first part of the network analysis, the focus is on node degrees. The degree of a node is the amount of edges that a node has in the network. Since this network is a undirected network, a weight can be added to the edges, so that the same edges are still counted multiple times. The degree distribution is found and illustrated to see whether the distribution is heavy-tailed, meaning that most nodes have few connections in the network and that there are hubs with a significantly degree than the average and median of the network. \n",
    "\n",
    "Regime:\n",
    "\n",
    "Looking into which regime the network belongs is an important analysis for understanding and characterizing the network. By doing simple calculations the regime and characteristics that follow can be determined. \n",
    "\n",
    "Visualization of the network:\n",
    "More realizations about the network can be obtained by simply visualizing the network. Firstly, the naturally formed communities and the specific hubs in the network can be identified by sizing the nodes by degree. Furthermore, the nodes can be colored according to a specific node attribute, e.g. country of birth, to see whether there are any connections in the network based on this attribute. \n",
    "\n",
    "- Assortativity (Numeric: degree, age, rating, popularity - Categorical: genre, birthplace, gender)\n",
    "The assortativity measure refers to the tendency of nodes to link to other nodes with the same (or similar for numeric values) attribute values, such as degree, age, gender etc. The assortativity coefficient ranges from -1 (perfect disassortative) to 1 (perfect assortative). A positive assortativity means that nodes with similar attribute values tend to connect, and a negative assortativity coefficient means that nodes with dissimilar values tend to form connections. As an example if the assortativity attribute is gender, in a perfect assortative network males would only connect to other males, and in a perfect dissasortative network males would only connect to females (in binary gendered network). \n",
    "\n",
    "- Closeness/eigenvector centrality\n",
    "\n",
    "\n",
    "* How did you use the tools to understand your dataset?\n",
    "##### Degree analysis\n",
    "After the network was created, the degrees of the nodes in the network were investigated. The average, median, mode, minimum and maximum degree were computed for both the weighted and unweighted network. With the edges being weighted, the average, median and max degree increases significantly, which is expected as popular actors may have a tendency of collaborating multiple times with other popular actors. The mode degree remains at 0, which is caused by the high number of isolates in the network. It is also worth noting, that purely based on the average and median degree, it can be seen that both the unweighted and weighted degree distribution are heavy-tailed as the median is significantly smaller than the average degree. This is further illustrated through two plots, where the distribution is plotted in normal scale and log-log-scale. On normal scale, it is clear to see the heavy-tailed distribution, which is further established through the linearity on the log-log-scale. \n",
    "\n",
    "Next, the highest degree nodes were found. The top 5 actors with highest unweighted degree illustrate the popular actors that have most unique collaborations, where the weighted degree illustrates the popular actors with with most collaborations in the popular actor network, this being both unique and non-unique collborations. For both the weighted and unweighted network, Samuel L. Jackson was at the top, and is thus being the most collaborative popular actor in the network. \n",
    "By looking the at most repeated edges (edges with highest weight), the \"best friends\" of the network was found. The top two collaborators in the network are Adam Copeland and Michael Hickenbottom, who are two wrestlers from the American WWE. It makes sense that these two have many collaborations as they have competed against eachother for many years. \n",
    "\n",
    "\n",
    "* Talk about how you’ve worked with text, including regular expressions, unicode, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Degrees and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute node degrees\n",
    "degrees = dict(G.degree())\n",
    "weighted_degrees = dict(G.degree(weight='weight'))\n",
    "\n",
    "# Compute the specificied values\n",
    "average_degree = np.mean(list(degrees.values()))\n",
    "median_degree = np.median(list(degrees.values()))\n",
    "try:\n",
    "    mode_degree = mode(list(degrees.values())) #Degree value that occurs with highest frequency among the nodes\n",
    "except:\n",
    "    mode_degree = \"No unique mode\"\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "# Same calculations but for STRENGTH (WEIGHTED DEGREE)\n",
    "average_weighted_degree = np.mean(list(weighted_degrees.values()))\n",
    "median_weighted_degree = np.median(list(weighted_degrees.values()))\n",
    "try:\n",
    "    mode_weighted_degree = mode(list(weighted_degrees.values()))\n",
    "except:\n",
    "    mode_weighted_degree = \"No unique mode\"\n",
    "min_weighted_degree = min(weighted_degrees.values())\n",
    "max_weighted_degree = max(weighted_degrees.values())\n",
    "\n",
    "print(\"Degree Analysis:\")\n",
    "print(f\"Average Degree: {average_degree}\")\n",
    "print(f\"Median Degree: {median_degree}\")\n",
    "print(f\"Mode Degree: {mode_degree}\")\n",
    "print(f\"Minimum Degree: {min_degree}\")\n",
    "print(f\"Maximum Degree: {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree Analysis:\")\n",
    "print(f\"Average Weighted Degree: {average_weighted_degree}\")\n",
    "print(f\"Median Weighted Degree: {median_weighted_degree}\")\n",
    "print(f\"Mode Weighted Degree: {mode_weighted_degree}\")\n",
    "print(f\"Minimum Weighted Degree: {min_weighted_degree}\")\n",
    "print(f\"Maximum Weighted Degree: {max_weighted_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the edges by weight in descending order\n",
    "sorted_edges = sorted(G.edges.data(), key=lambda x: x[2]['weight'], reverse=True)\n",
    "most_connected_actors = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "most_connected_actors_weighted = sorted(weighted_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 5 actors with the highest degree:\")\n",
    "for i, actor in enumerate(most_connected_actors[:5], start=1):\n",
    "    print(f\"{i}: Degree: {actor[1]},\\t ({G.nodes[actor[0]]['main_genre']})\\t {actor[0]}\")\n",
    "\n",
    "print(\"\\nTop 5 actors with the highest weighted degree:\")\n",
    "for i, actor in enumerate(most_connected_actors_weighted[:5], start=1):\n",
    "    print(f\"{i}: Degree: {actor[1]},\\t ({G.nodes[actor[0]]['main_genre']})\\t {actor[0]}\")\n",
    "\n",
    "print(\"\\nTop 5 most important edges:\")\n",
    "for i, edge in enumerate(sorted_edges[:5], start=1):\n",
    "    print(f\"{i}: Degree: {edge[2]['weight']}, ({G.nodes[edge[0]]['main_genre']} - {G.nodes[edge[1]]['main_genre']}) \\t {edge[0]} - {edge[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top ten actors who have participated the most together, as the weight of an edge represents the number of times two actors (nodes) have co-acted in movies together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree distribution\n",
    "degree_sequence = sorted([d for n, d in G.degree()], reverse=False)  # sort in ascending order\n",
    "degree_count = nx.degree_histogram(G)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# degree distribution\n",
    "axs[0].bar(range(len(degree_count)), degree_count, align='center')\n",
    "axs[0].axvline(average_degree, color='r', linestyle='-', label='Average Degree')\n",
    "axs[0].set_xlabel('Degree')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_title('Degree Distribution')\n",
    "axs[0].legend()\n",
    "\n",
    "# loglog degree distribution\n",
    "axs[1].bar(range(len(degree_count)), degree_count, align='center')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('Degree')\n",
    "axs[1].set_ylabel('Count')\n",
    "axs[1].set_title('Degree Distribution (log scale)')\n",
    "axs[1].axvline(average_degree, color='r', linestyle='-', label='Average Degree')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability that makes the expected number of edges equal to the actual number of edges in the graph.\n",
    "p = 2*L/(N*(N-1))\n",
    "print(\"Prob:\",p)\n",
    "\n",
    "# Compute the natural logarithm of N\n",
    "ln_N = np.log(N)\n",
    "\n",
    "print(f\"ln(N): {ln_N:.2f} < k: {average_degree:.2f}\\nAnd p: {p:.5f} > ln(N)/N: {ln_N/N:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the average degree is 10.22 and thus <k> > 1, and it is larger than ln(N) while p > ln(N)/N, the network must fall into the connected regime. It is therefore above the critical threshold. However, as we have thresholded the network by removing edges with an weight less than 2, it is not fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with netwulf\n",
    "# Style configuration\n",
    "config = {\n",
    "    'zoom': 1,\n",
    "    'node_charge': -40,\n",
    "    'node_gravity': 0.8,\n",
    "    'link_distance': 50,\n",
    "    'link_distance_variation': 1,\n",
    "    'node_collision': True,\n",
    "    'wiggle_nodes': False,\n",
    "    'freeze_nodes': False,\n",
    "\n",
    "    'node_size': 15,\n",
    "    'node_stroke_width': 0.3,\n",
    "    'node_size_variation': 0.9,\n",
    "    'display_node_labels': False,\n",
    "    'scale_node_size_by_strength': True,\n",
    "\n",
    "    'link_width': 2,\n",
    "    'link_width_variation': 2,\n",
    "    'link_alpha': 0.05,\n",
    "\n",
    "}\n",
    "\n",
    "G_plot_nation = G.copy()\n",
    "\n",
    "\n",
    "for k, v in G_plot_nation.nodes(data=True):\n",
    "    v['group'] = v['birthplace']; del v['birthplace']\n",
    "\n",
    "\n",
    "for n, data in G_plot_nation.nodes(data=True):\n",
    "    data['size'] = np.random.random()\n",
    "\n",
    "\n",
    "for n1, n2, data in G_plot_nation.edges(data=True):\n",
    "    data['weight'] = np.random.random()\n",
    "\n",
    "\n",
    "network, config = nw.visualize(G_plot_nation, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the color of all nodes in the network and find the top 10 most used colors\n",
    "node_colors = [node['color'] for node in network['nodes']]\n",
    "\n",
    "# Count the frequency of each color\n",
    "color_counts = pd.Series(node_colors).value_counts()\n",
    "\n",
    "# Get the top 10 most used colors\n",
    "top_10_colors = color_counts.head(10)\n",
    "\n",
    "# Create color dictionary\n",
    "color_dict = {}\n",
    "\n",
    "# Go through the nodes and assign the color to the birthplace\n",
    "for color in top_10_colors.index:\n",
    "    for node in network['nodes']:\n",
    "        if node['color'] == color:\n",
    "            color_dict[color] = df_actors_filtered[df_actors_filtered['actor']==node['id']]['birthplace'].values[0]\n",
    "            break\n",
    "\n",
    "# Print the color dictionary\n",
    "for color, birthplace in color_dict.items():\n",
    "    print(f\"Color: {color}, Birthplace: {birthplace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [mpatches.Patch(color=color, label=f\"{color_dict[color]}\") for color in color_dict]\n",
    "\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.legend(handles=patches, loc='center', frameon=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = nw.draw_netwulf(network)\n",
    "plt.legend(handles=patches, loc='upper left', frameon=False)\n",
    "fig.set_size_inches(12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved png-file \"country_graph.png\" and plot it in the notebook\n",
    "Image(filename='country_graph3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree assortativity coefficient\n",
    "degree_assortativity = nx.degree_assortativity_coefficient(G)\n",
    "\n",
    "# Calculate the attribute assortativity coefficient\n",
    "country_assortativity = nx.attribute_assortativity_coefficient(G, 'birthplace')\n",
    "\n",
    "gender_assortativity = nx.attribute_assortativity_coefficient(G, 'gender')\n",
    "\n",
    "# Calculate the numeric assortativity coefficient\n",
    "age_assortativity = nx.numeric_assortativity_coefficient(G, 'age')\n",
    "\n",
    "# Calcualte the assortativity for main_genre\n",
    "main_genre_assortativity = nx.attribute_assortativity_coefficient(G, 'main_genre')\n",
    "\n",
    "rating_assortativity = nx.numeric_assortativity_coefficient(G, 'avg_rating')\n",
    "popularity_assortativity = nx.numeric_assortativity_coefficient(G, 'popularity')\n",
    "\n",
    "print(f\"Categorical:\\nGenre assortativity: {main_genre_assortativity}, \\nBirth place assortativity: {country_assortativity}, \\nGender assortativity: {gender_assortativity}\")\n",
    "\n",
    "print(f\"\\nNumeric:\\nDegree assortativity: {degree_assortativity}, \\nAge assortativity: {age_assortativity}, \\nRating assortativity: {rating_assortativity}, \\nPopularity assortativity: {popularity_assortativity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node to country assortativity: expected a higher value, but the litl lower value can be due to several reasons, desired nationality diversity, moving to different countries (living place) etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the isolates\n",
    "isolates = list(nx.isolates(G))\n",
    "\n",
    "# Print the number of isolates\n",
    "print(len(isolates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_movies_filtered['rating']\n",
    "# x2 = df_movies_filtered['popularity']\n",
    "# plt.scatter(x,x2) #ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the closeness centrality of the network\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Sort the actors according to the closeness centrality\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most central actors\n",
    "most_central_actors = sorted_closeness_centrality[:10]\n",
    "most_central_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the eigenvector centrality of the network\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Sort the scientists according to the eigenvector centrality\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most central actors\n",
    "most_central_actors2 = sorted_eigenvector_centrality[:10]\n",
    "most_central_actors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closeness centrality vs node degree to see if there is a correlation\n",
    "closeness_centrality_values = list(closeness_centrality.values())\n",
    "eigenvector_centrality_values = list(eigenvector_centrality.values())\n",
    "degree_values = list(dict(G.degree()).values())\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# plot the closeness centrality vs node degree to see if there is a correlation\n",
    "axs[0].scatter(degree_values, closeness_centrality_values, alpha=0.5)\n",
    "axs[0].set_xlabel('Node Degree')\n",
    "axs[0].set_ylabel('Closeness Centrality')\n",
    "axs[0].set_title('Closeness Centrality vs Node Degree')\n",
    "\n",
    "# plot the eigenvector centrality vs node degree to see if there is a correlation\n",
    "axs[1].scatter(degree_values, eigenvector_centrality_values, alpha=0.5)\n",
    "axs[1].set_xlabel('Node Degree')\n",
    "axs[1].set_ylabel('Eigenvector Centrality')\n",
    "axs[1].set_title('Eigenvector Centrality vs Node Degree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used for tokenization of movie abstracts, including cleaning, stemming and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# Define a function to tokenize and clean text\n",
    "def tokenize_and_clean_text3(text, collocations={}, with_collocations=False): #takes text and dictionary of collocations\n",
    "    # Exclude URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Exclude mathematical symbols and numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Exclude punctuation and convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text).lower()\n",
    "\n",
    "    # Tokenize\n",
    "    if with_collocations:\n",
    "        tokenizer = MWETokenizer(list(collocations.keys()))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = tokenizer.tokenize(tokens)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # stem\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('english')]# TO REMOVE STOPWORDS \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First clean text\n",
    "df_movies_filtered['tokens'] = df_movies_filtered['abstract'].apply(lambda x: tokenize_and_clean_text3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "# Initialize an empty list to store all bigrams\n",
    "all_bigrams = []\n",
    "\n",
    "\n",
    "# For each list of tokens in the 'tokens' column\n",
    "for tokens in df_movies_filtered['tokens']:\n",
    "    # Generate the list of bigrams\n",
    "    bigrams_list = list(bigrams(tokens))\n",
    "    # Add the bigrams to the list of all bigrams\n",
    "    all_bigrams.extend(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from scipy import stats\n",
    "\n",
    "all_unique_bigrams = list(set(all_bigrams))\n",
    "tokens_list = list(df_movies_filtered['tokens'].copy().explode())\n",
    "\n",
    "# Create a Counter for bigrams and tokens\n",
    "bigrams_counter = collections.Counter(all_bigrams)\n",
    "tokens_counter = collections.Counter(tokens_list)\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# For each unique pair of words\n",
    "for w1, w2 in tqdm(all_unique_bigrams):\n",
    "    # number of times w1 and w2 appear together in all_bigrams\n",
    "    nii = bigrams_counter[(w1, w2)]\n",
    "    # number of times w1 appears without w2\n",
    "    noi = tokens_counter[w1] - nii\n",
    "    # number of times w2 appears without w1\n",
    "    nio = tokens_counter[w2] - nii\n",
    "    # number of times w1 and w2 do not appear together\n",
    "    noo = len(all_bigrams) - nii - noi - nio\n",
    "\n",
    "    O = [[nii, nio], [noi, noo]]\n",
    "\n",
    "    R1 = nii + nio\n",
    "    C1 = nii + noi\n",
    "    R2 = noi + noo\n",
    "    C2 = nio + noo\n",
    "    N = R1 + R2 + C1 + C2\n",
    "\n",
    "    E = [[R1*C1/N, R1*C2/N],[R2*C1/N, R2*C2/N]] \n",
    "\n",
    "    X_sq = sum((O[i][j] - E[i][j]) ** 2 / E[i][j] for i in range(len(O)) for j in range(len(O[i])))\n",
    "    p_val = stats.chi2.sf(X_sq, df=1)\n",
    "\n",
    "    data[(w1, w2)] = O, E, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocations = {}\n",
    "\n",
    "for w1, w2 in tqdm(all_unique_bigrams):\n",
    "    if bigrams_counter[(w1, w2)] > 50 and data[(w1, w2)][2] < 0.001:\n",
    "        collocations.update({(w1, w2): bigrams_counter[(w1, w2)]})\n",
    "\n",
    "print(f\"\\nNumber of collocations: {len(collocations)}\")\n",
    "# Print out the top 20 of them by number of occurrences\n",
    "sorted(collocations.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recomputing the tokens with collocations\n",
    "df_movies_filtered['tokens'] = df_movies_filtered['abstract'].apply(lambda x: tokenize_and_clean_text3(x, collocations, \n",
    "                                                                                                       with_collocations=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the dataframe as a csv file called df_movies_tokens\n",
    "# df_movies_filtered.to_csv('data/df_movies_tokens.csv', index=False)\n",
    "\n",
    "df_movies_tokens = pd.read_csv('data/df_movies_tokens.csv', converters={'tokens': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the best partition using the Louvain method\n",
    "partition = community_louvain.best_partition(G, resolution=0.5)\n",
    "\n",
    "# Compute the modularity of this partition\n",
    "modularity = community_louvain.modularity(partition, G)\n",
    "\n",
    "print(\"Modularity found by Louvain algorithm:\", modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of communities and their sizes\n",
    "communities = {}\n",
    "for node, community in partition.items():\n",
    "    if community not in communities:\n",
    "        communities[community] = []\n",
    "    communities[community].append(node)\n",
    "\n",
    "num_communities = len(communities)\n",
    "community_sizes = sorted([len(nodes) for nodes in communities.values()], reverse=True)\n",
    "\n",
    "print(\"Number of communities:\", num_communities)\n",
    "print(\"Sizes of communities:\", community_sizes)\n",
    "# Check if the modularity is significantly different than 0\n",
    "if abs(modularity) > 0.01:  # or any other threshold you consider significant\n",
    "    print(\"The modularity is significantly different than 0.\")\n",
    "else:\n",
    "    print(\"The modularity is not significantly different than 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the partition and degree information\n",
    "df_communities = pd.DataFrame({\n",
    "    'actor': list(G.nodes),\n",
    "    'community': [partition[node] for node in G.nodes],\n",
    "    'degree': [G.degree(node) for node in G.nodes]\n",
    "})\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df_communities.to_csv('data/communities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the df_communities csv-file\n",
    "df_communities = pd.read_csv('data/communities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge actors_works_df with communities_df\n",
    "df = pd.merge(df_actors_filtered, df_communities, on='actor')\n",
    "df = df.drop(columns=[\"movies\"]).explode('movie_IDs')\n",
    "\n",
    "\n",
    "# Merge with abstracts_df\n",
    "df = pd.merge(df, df_movies_tokens, left_on='movie_IDs', right_on='movie_ID')\n",
    "\n",
    "# # Get abstract tokens for all communities\n",
    "all_communities_abstracts = df.groupby('community')['tokens'].agg(\"sum\").reset_index()\n",
    "\n",
    "all_communities_abstracts.columns = ['Community', 'Abstract Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 9 communities and filter the abstracts for these communities\n",
    "top9_communities = df_communities['community'].value_counts().nlargest(9).index\n",
    "top9_communities_abstracts = all_communities_abstracts[all_communities_abstracts['Community'].isin(top9_communities)]\n",
    "\n",
    "top_terms = {}\n",
    "top_tfidf_terms = {}\n",
    "all_tfidf_terms = {}\n",
    "\n",
    "# (for less computational cost) get IDF for each term once and store the results\n",
    "idf_dict = {}\n",
    "for term in tqdm(set.union(*top9_communities_abstracts['Abstract Tokens'].apply(set))):\n",
    "    idf_dict[term] = math.log(len(top9_communities_abstracts) / \n",
    "                              sum(term in abstract for abstract in top9_communities_abstracts['Abstract Tokens']))\n",
    "\n",
    "\n",
    "for community in tqdm(top9_communities):\n",
    "    # Get abstract tokens for the community\n",
    "    abstract_tokens = top9_communities_abstracts[top9_communities_abstracts['Community'] == community]['Abstract Tokens'].values[0]\n",
    "    \n",
    "    # The top 10 TF terms\n",
    "    term_counts = Counter(abstract_tokens)\n",
    "    top_terms[community] = term_counts.most_common(10)\n",
    "\n",
    "\n",
    "    tfidf = {}\n",
    "    for term, count in term_counts.items():\n",
    "        # TF for all terms in top 9 communities\n",
    "        tf = count / len(abstract_tokens)\n",
    "\n",
    "        # Get IDF from the precalculated dictionary\n",
    "        idf = idf_dict[term]\n",
    "        \n",
    "        # TF-IDF for all terms in top 9 communities\n",
    "        tfidf[term] = tf * idf\n",
    "\n",
    "    all_tfidf_terms[community] = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top 10 TF-IDF words\n",
    "    top_tfidf_terms[community] = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_tfidf_terms.items():\n",
    "    print(f\"Community {item[0]}:\")\n",
    "    print(f\"Top 10 TF-IDF terms: {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top9_communities_df = df_communities[df_communities['community'].isin(top9_communities)]\n",
    "\n",
    "# Group by Community and Actor, and sum the Degree to get the total Degree for each Actor in each Community\n",
    "grouped_df = top9_communities_df.groupby(['community', 'actor'])['degree'].sum().reset_index()\n",
    "\n",
    "# Make new dataframe storing the top 3 actors by degree for each community\n",
    "top_actors = grouped_df.groupby('community').apply(lambda x: x.nlargest(5, 'degree')).reset_index(drop=True)\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all_tfidf_terms as a JSON file in the data folder\n",
    "with open('data/all_tfidf_terms.json', 'w') as f:\n",
    "    json.dump(all_tfidf_terms, f)\n",
    "\n",
    "# Save top9_communities as a CSV file in the data folder\n",
    "pd.DataFrame(top9_communities, columns=['community']).to_csv('data/top9_communities.csv', index=False)\n",
    "\n",
    "# Save top_actors as a CSV file in the data folder\n",
    "top_actors.to_csv('data/top_actors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files for wordcloud\n",
    "top_actors = pd.read_csv('data/top_actors.csv')\n",
    "\n",
    "top9_communities = pd.read_csv('data/top9_communities.csv')\n",
    "\n",
    "with open('data/all_tfidf_terms.json', 'r') as file:\n",
    "    all_tfidf_terms = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(24, 24))\n",
    "\n",
    "for i, community in enumerate(top9_communities[\"community\"]):\n",
    "    # Get the already calculated top TF-IDF terms for the community\n",
    "    tfidf_terms = dict(all_tfidf_terms[str(community)])\n",
    "    \n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = None, \n",
    "                min_font_size = 10).generate_from_frequencies(tfidf_terms)\n",
    "    \n",
    "    # Subplot indices\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    # Plot the word cloud\n",
    "    axs[row, col].imshow(wordcloud)\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "    # Get the names of the top three actors in the community\n",
    "    top_three_actors = top_actors[top_actors['community'] == community]['actor'].values\n",
    "    # Create the title string with newlines\n",
    "    title_string = \"\\n\".join(top_three_actors)\n",
    "\n",
    "    # Set title of subplot to the names of the top three actors, each on a new line\n",
    "    axs[row, col].set_title(f\"Top actors in community {community}:\\n{title_string}\", fontsize=15)\n",
    "\n",
    "# Remove any extra subplots\n",
    "for j in range(i+1, 9):\n",
    "    fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "plt.tight_layout(pad = 1) \n",
    "plt.subplots_adjust(hspace = 0.25)  # Increase the hspace value\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_actors_filtered with communities df\n",
    "df_actors_communities = pd.merge(df_actors_filtered, df_communities, on='actors')\n",
    "\n",
    "# For community in top20_communities, calculate community attributes\n",
    "# Extract top 20 communities\n",
    "top20_communities_idx = df_communities['community'].value_counts().nlargest(10).index\n",
    "top20_community_actors = df_actors_communities[df_actors_communities['community'].isin(top20_communities_idx)]\n",
    "\n",
    "# Calculate in each community (group_by)\n",
    "def average_of_list(x):\n",
    "    return np.mean([val for sublist in x for val in sublist])\n",
    "def sum_dicts(dicts):\n",
    "    result = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if k in result:\n",
    "                result[k] += v\n",
    "            else:\n",
    "                result[k] = v\n",
    "    return result\n",
    "top20_community_attributes = top20_community_actors.groupby('community').agg({\n",
    "    'actors': 'count',\n",
    "    'ages': 'mean',\n",
    "    'ratings': [('mean', average_of_list)],\n",
    "    'degree': [('mean', 'mean')],\n",
    "    'birthplaces': lambda x: x.mode().iloc[0] if not x.mode().empty else None,\n",
    "})\n",
    "\n",
    "# Clean columns\n",
    "top20_community_attributes.columns = ['_'.join(col).strip() for col in top20_community_attributes.columns.values]\n",
    "top20_community_attributes.reset_index(inplace=True)\n",
    "\n",
    "# Calculate main genre in each community\n",
    "df_grouped = df_actors_communities.groupby('community')['genres'].apply(sum_dicts).reset_index()\n",
    "df_grouped = df_grouped.dropna()\n",
    "df_grouped = df_grouped.loc[df_grouped.groupby('community')['genres'].idxmax()]\n",
    "top20_community_attributes = pd.merge(top20_community_attributes, df_grouped, on='community', how='inner')\n",
    "top20_community_attributes = top20_community_attributes.drop(columns=['genres'])\n",
    "\n",
    "# Rename columns\n",
    "top20_community_attributes = top20_community_attributes.rename(columns={'birthplaces_<lambda>': 'country'})\n",
    "top20_community_attributes = top20_community_attributes.rename(columns={'level_1': 'main_genre'})\n",
    "\n",
    "top20_community_attributes.sort_values(by='actors_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot success vs (genre, country, age)\n",
    "fig, ax = plt.subplots(1,3, figsize=(16, 4))\n",
    "\n",
    "ax[0].set_title('Age vs Success')\n",
    "ax[0].scatter(top20_community_attributes['ages_mean'], top20_community_attributes['ratings_mean'])\n",
    "ax[0].set_xlabel('average age')\n",
    "ax[0].set_ylabel('average success')\n",
    "\n",
    "ax[1].set_title('Genre vs Success')\n",
    "ax[1].boxplot([top20_community_attributes.loc[top20_community_attributes['main_genre'] == genre, 'ratings_mean'] for genre in top20_community_attributes['main_genre'].unique()])\n",
    "ax[1].set_xlabel('Genre')\n",
    "ax[1].set_ylabel('average success')\n",
    "ax[1].set_xticklabels(top20_community_attributes['main_genre'].unique(), rotation=80)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "ax[2].set_title('Country vs Success')\n",
    "ax[2].boxplot([top20_community_attributes.loc[top20_community_attributes['country'] == country, 'ratings_mean'] for country in top20_community_attributes['country'].unique()])\n",
    "ax[2].set_xlabel('Country')\n",
    "ax[2].set_ylabel('average success')\n",
    "ax[2].set_xticklabels(top20_community_attributes['country'].unique(), rotation=80)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "* What went well?\n",
    "* What is still missing? What could be improved? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
