{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "api_key = \"f5813332cb558d374cbcb057ea2fc48b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def movie_titles_and_IDs_from_actor_ID(actor_id, session):\n",
    "    url = f\"https://api.themoviedb.org/3/person/{actor_id}/movie_credits?api_key={api_key}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "    # Return the whole movie dictionary, not just the title\n",
    "    return [movie['title'] for movie in data['cast']], [movie['id'] for movie in data['cast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(page, session):\n",
    "    url = f\"https://api.themoviedb.org/3/person/popular?api_key={api_key}&page={page}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "    people = []\n",
    "    for person in data['results']:\n",
    "        person_url = f\"https://api.themoviedb.org/3/person/{person['id']}?api_key={api_key}\"\n",
    "        person_response = session.get(person_url)\n",
    "        person_data = person_response.json()\n",
    "        people.append((person['name'], person['id'], person_data['gender'], person_data['birthday'], person_data['place_of_birth']))\n",
    "    return people\n",
    "\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        pages = list(range(1, 251))\n",
    "        fetch_page_with_session = partial(fetch_page, session=session)\n",
    "        people = list(executor.map(fetch_page_with_session, pages))\n",
    "\n",
    "all_people_names, all_people_ids, all_people_genders, all_people_birthdays, all_people_birthplaces = zip(*itertools.chain(*people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with 'actors' column\n",
    "df_actors = pd.DataFrame(all_people_names, columns=['actors'])\n",
    "\n",
    "# Add 'ids', 'genders', and 'birthplaces' columns to the DataFrame\n",
    "df_actors['actor_ids'] = all_people_ids\n",
    "df_actors['genders'] = all_people_genders\n",
    "df_actors['ages'] = all_people_birthdays # This is not the age, but the birthday\n",
    "df_actors['birthplaces'] = all_people_birthplaces\n",
    "\n",
    "# Change birthplaces so that it only contains the country (text after the last comma)\n",
    "df_actors['birthplaces'] = df_actors['birthplaces'].str.split(',').str[-1]\n",
    "\n",
    "# Change birthday to age\n",
    "df_actors['ages'] = pd.to_datetime(df_actors['ages'], errors='coerce')\n",
    "df_actors['ages'] = (pd.to_datetime('today') - df_actors['ages']).dt.days // 365\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_actors.dropna(inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df_actors.drop_duplicates(subset='actors', inplace=True)\n",
    "#reset index\n",
    "df_actors.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch movies for each actor and add them to 'movies' and 'movie_IDs' columns\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    movie_titles_and_ids_from_actor_ID_with_session = partial(movie_titles_and_IDs_from_actor_ID, session=session)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        movies_and_ids = list(executor.map(movie_titles_and_ids_from_actor_ID_with_session, df_actors['actor_ids']))\n",
    "\n",
    "# Add 'movies' column to df_actors\n",
    "df_actors['movies'] = [x[0] for x in movies_and_ids]\n",
    "\n",
    "# Flatten movies_and_ids into two separate lists\n",
    "movies = [item for sublist in [x[0] for x in movies_and_ids] for item in sublist]\n",
    "ids = [item for sublist in [x[1] for x in movies_and_ids] for item in sublist]\n",
    "\n",
    "# Convert the lists into a list of dictionaries\n",
    "movies_and_ids_dict = [{'movie': movie, 'movie_ID': id} for movie, id in zip(movies, ids)]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "movies_df = pd.DataFrame(movies_and_ids_dict)\n",
    "movies_df = movies_df.drop_duplicates()\n",
    "movies_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(session, url):\n",
    "    future = session.get(url, headers=headers)\n",
    "    return future\n",
    "\n",
    "def fetch_all(urls):\n",
    "    with FuturesSession() as session:\n",
    "        futures = [fetch(session, url) for url in urls]\n",
    "        responses = [future.result().json() for future in futures]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the URLs\n",
    "urls = [f\"https://api.themoviedb.org/3/movie/{id}?api_key={api_key}\" for id in movies_df[\"movie_ID\"]]\n",
    "print('urls are prepared')\n",
    "\n",
    "# Fetch all responses\n",
    "responses = fetch_all(urls)\n",
    "print('all responses are fetched')\n",
    "\n",
    "# Process the responses\n",
    "for i, response in enumerate(responses):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing response {i+1}/{len(responses)}\")\n",
    "    if isinstance(response, Exception):\n",
    "        print(f\"Error: {response}\")\n",
    "        continue  # Skip this response\n",
    "    # Process the response here\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "ratings = []\n",
    "popularities = []\n",
    "genres = []\n",
    "release_dates = []\n",
    "abstracts = []\n",
    "\n",
    "# Process the responses one by one\n",
    "for data in tqdm(responses):\n",
    "    ratings.append(data.get('vote_average'))\n",
    "    popularities.append(data.get('popularity'))\n",
    "    genres.append([genre['name'] for genre in data.get('genres', [])])\n",
    "    release_dates.append(data.get('release_date'))\n",
    "    abstracts.append(data.get('overview'))\n",
    "    \n",
    "# Assign the lists to the DataFrame columns\n",
    "movies_df['rating'] = ratings\n",
    "movies_df['popularity'] = popularities\n",
    "movies_df['genres'] = genres\n",
    "movies_df['release_date'] = release_dates\n",
    "movies_df['abstract'] = abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "movies_df.drop_duplicates(subset='movie', inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "movies_df.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with empty lists in 'genres' column\n",
    "movies_df = movies_df[movies_df['genres'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Remove rows with empty release dates\n",
    "movies_df = movies_df[movies_df['release_date'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if abstract is missing\n",
    "movies_df = movies_df[movies_df['abstract'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if rating is missing\n",
    "movies_df = movies_df[movies_df['rating'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Drop row if popularity is missing\n",
    "movies_df = movies_df[movies_df['popularity'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Reset index\n",
    "movies_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes so that each actor is associated with the movies they have acted in\n",
    "df_actors_filtered = df_actors.copy()\n",
    "df_movies_filtered = movies_df.copy()\n",
    "df_movies_filtered.rename(columns={'movie': 'movies'}, inplace=True)\n",
    "\n",
    "df_actors_filtered = df_actors_filtered.explode('movies').reset_index(drop=True)\n",
    "\n",
    "df_actors_movies = df_actors_filtered.merge(df_movies_filtered, on='movies', how='left')\n",
    "\n",
    "# Remove all rows where the release date is before 2010\n",
    "df_actors_movies = df_actors_movies[(df_actors_movies['release_date'] >= '2010-01-01') & (df_actors_movies['release_date'] <= '2024-04-16')]\n",
    "\n",
    "# Collapse the the actors in the actor column so there is only one row per actor and the movies and movie_IDs are stored in lists\n",
    "df_actors_filtered = df_actors_movies.groupby('actors').agg({'actor_ids': 'first',\n",
    "                                                             'genders': 'first',\n",
    "                                                             'birthplaces': 'first',\n",
    "                                                             'ages': 'first',\n",
    "                                                             'movies': list, \n",
    "                                                             'movie_ID': list}).reset_index()\n",
    "\n",
    "df_movies_filtered = df_actors_movies.drop_duplicates(subset=['movies']).groupby('movies').agg({'movie_ID': 'first',\n",
    "                                                                                               'rating': 'first',\n",
    "                                                                                               'popularity': 'first',\n",
    "                                                                                               'genres': 'first',\n",
    "                                                                                               'release_date': 'first',\n",
    "                                                                                               'abstract': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import re\n",
    "\n",
    "# Clean birthplace column in actor dataframe\n",
    "len_bef_clean = len(df_actors_filtered['birthplaces'].unique())\n",
    "\n",
    "country_names = [country.name for country in pycountry.countries]\n",
    "\n",
    "def normalize_country_name(name):\n",
    "    # strip name\n",
    "    name = name.strip()\n",
    "    # replace '.' with ''\n",
    "    name = name.replace('.', '').replace(']', '')\n",
    "    if \"Türkiye\" in name or \"Turkey\" in name:\n",
    "        return \"Turkey\"\n",
    "    try:\n",
    "        # Try to get the country object\n",
    "        country = pycountry.countries.get(name=name)\n",
    "        if country is not None:\n",
    "            # If the country object is found, return the official name\n",
    "            return country.name\n",
    "        else:\n",
    "            # If the country object is not found, try to find it by its common name\n",
    "            country = pycountry.countries.search_fuzzy(name)\n",
    "            return country[0].name\n",
    "    except LookupError:\n",
    "        # Standardizing names\n",
    "        for country_name in country_names:\n",
    "            if country_name in name:\n",
    "                return country_name\n",
    "\n",
    "        # Fixing abbreviations and wird instances\n",
    "        if \"Russia\" in name:\n",
    "            return \"Russian Federation\"\n",
    "        elif \"USA\" in name or \" US\" in name or \"United States\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Korea\" in name:\n",
    "            return \"Korea, Republic of\"\n",
    "        elif \"UK\" in name or \"İngiltere\" in name:\n",
    "            return \"United Kingdom\"\n",
    "        elif \"Czech\" in name:\n",
    "            return \"Czechia\"\n",
    "        # Hardcoded, could use package to translate, maybe not necessary, few occurences\n",
    "        elif \"TX\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Frankrike\" in name:\n",
    "            return \"France\"\n",
    "        elif \"Afrique du Sud\" in name:\n",
    "            return \"South Africa\"\n",
    "        elif \"Irlanda\" in name:\n",
    "            return \"Ireland\"\n",
    "        elif \"中国\" in name or \"中华民国\" in name or \"重庆\" in name or \"南京\" in name:\n",
    "            return \"China\"\n",
    "        \n",
    "        \n",
    "        # Updating from old names\n",
    "        if \"now\" in name:\n",
    "            match = re.search(r'\\[now (.*?)', name)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        # If the country is not found, return the original name\n",
    "        return name\n",
    "\n",
    "# Normalize the country names in the DataFrame\n",
    "df_actors_filtered['birthplaces'] = df_actors_filtered['birthplaces'].apply(normalize_country_name)\n",
    "\n",
    "len_aft_clean = len(df_actors_filtered['birthplaces'].unique())\n",
    "print(\"length before cleaning\", len_bef_clean)\n",
    "print(\"length after cleaning\", len_aft_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv files\n",
    "df_actors_filtered.to_csv('data/actors.csv', index=False)\n",
    "df_movies_filtered.to_csv('data/movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes from csv files\n",
    "df_actors_filtered = pd.read_csv('data/actors.csv', converters={'movies': ast.literal_eval, 'movie_ID': ast.literal_eval})\n",
    "df_movies_filtered = pd.read_csv('data/movies.csv', converters={'genres': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "movie_to_actors = defaultdict(list)\n",
    "\n",
    "for _ , row in df_actors_filtered.iterrows():\n",
    "    actor = row['actors']\n",
    "    movies = row['movies']\n",
    "    for movie in movies:\n",
    "        movie_to_actors[movie].append(actor)\n",
    "\n",
    "edges = defaultdict(int)\n",
    "for actors in movie_to_actors.values():\n",
    "    for pair in itertools.combinations(sorted(actors), 2):\n",
    "        edges[pair] += 1\n",
    "\n",
    "# Step 2: Create the Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "for _ , row in df_actors_filtered.iterrows():\n",
    "    G.add_node(row['actors'], gender=row['genders'], age=row['ages'], birthplace=row['birthplaces'])\n",
    "\n",
    "# Add edges with weights\n",
    "for edge, weight in edges.items():\n",
    "\n",
    "# YAHNI\n",
    "    if weight >= 2:\n",
    "        G.add_edge(*edge, weight=weight)\n",
    "\n",
    "# Step 3: Analyze the Graph\n",
    "degrees = dict(G.degree(weight='weight'))\n",
    "most_connected_actors = sorted(degrees.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Degrees and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the edges by weight in descending order\n",
    "sorted_edges = sorted(G.edges.data(), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "print(\"Top 10 most important edges:\")\n",
    "for i, edge in enumerate(sorted_edges[:10], start=1):\n",
    "    print(f\"Rank: {i}, Edge: {edge[0]} - {edge[1]}, Weight: {edge[2]['weight']}\")\n",
    "\n",
    "print(\"\\nTop 10 actors with the highest degree:\")\n",
    "for i, actor in enumerate(most_connected_actors[:10], start=1):\n",
    "    print(f\"Rank: {i}, Actor: {actor[0]}, Degree: {actor[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top ten actors who have participated the most together, as the weight of an edge represents the number of times two actors (nodes) have co-acted in movies together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Compute node degrees\n",
    "degrees = dict(G.degree())\n",
    "weighted_degrees = dict(G.degree(weight='weight'))\n",
    "\n",
    "# Compute the specificied values\n",
    "average_degree = np.mean(list(degrees.values()))\n",
    "median_degree = np.median(list(degrees.values()))\n",
    "try:\n",
    "    mode_degree = mode(list(degrees.values())) #Degree value that occurs with highest frequency among the nodes\n",
    "except:\n",
    "    mode_degree = \"No unique mode\"\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "# Same calculations but for STRENGTH (WEIGHTED DEGREE)\n",
    "average_weighted_degree = np.mean(list(weighted_degrees.values()))\n",
    "median_weighted_degree = np.median(list(weighted_degrees.values()))\n",
    "try:\n",
    "    mode_weighted_degree = mode(list(weighted_degrees.values()))\n",
    "except:\n",
    "    mode_weighted_degree = \"No unique mode\"\n",
    "min_weighted_degree = min(weighted_degrees.values())\n",
    "max_weighted_degree = max(weighted_degrees.values())\n",
    "\n",
    "print(\"Degree Analysis:\")\n",
    "print(f\"Average Degree: {average_degree}\")\n",
    "print(f\"Median Degree: {median_degree}\")\n",
    "print(f\"Mode Degree: {mode_degree}\")\n",
    "print(f\"Minimum Degree: {min_degree}\")\n",
    "print(f\"Maximum Degree: {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree Analysis:\")\n",
    "print(f\"Average Weighted Degree: {average_weighted_degree}\")\n",
    "print(f\"Median Weighted Degree: {median_weighted_degree}\")\n",
    "print(f\"Mode Weighted Degree: {mode_weighted_degree}\")\n",
    "print(f\"Minimum Weighted Degree: {min_weighted_degree}\")\n",
    "print(f\"Maximum Weighted Degree: {max_weighted_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree distribution\n",
    "degree_sequence = sorted([d for n, d in G.degree()], reverse=False)  # sort in ascending order\n",
    "degree_count = nx.degree_histogram(G)\n",
    "\n",
    "# Plot degree distribution\n",
    "plt.bar(range(len(degree_count)), degree_count, align='center')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Degree Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a log plot\n",
    "plt.bar(range(len(degree_count)), degree_count, align='center')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Degree Distribution (log scale)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with netwulf\n",
    "import netwulf as nw\n",
    "\n",
    "nw.visualize(G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree assortativity coefficient\n",
    "assortativity = nx.degree_assortativity_coefficient(G)\n",
    "\n",
    "assortativity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
