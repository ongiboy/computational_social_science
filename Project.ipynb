{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Assignment B\n",
    "Link to git repository: https://github.com/ongiboy/Cognitive-Social-Science\n",
    "\n",
    "Group members:\n",
    "* Christian Ong Hansen (s204109)\n",
    "* Kavus Latifi Yaghin (s214601)\n",
    "* Daniel Damkjær Ries (s214641)\n",
    "\n",
    "Group member's contribution:\n",
    "* Every task was made in collaboration by all members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is your dataset\n",
    "* Why did you choose this/these particular dataset(s)?\n",
    "* What was your goal for the end user’s experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats\n",
    "* Write about your choices in data cleaning and preprocessing\n",
    "* Write a short section that discusses the dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval and cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "import json\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "api_key = \"f5813332cb558d374cbcb057ea2fc48b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def movie_titles_and_IDs_from_actor_ID(actor_id, session):\n",
    "    url = f\"https://api.themoviedb.org/3/person/{actor_id}/movie_credits?api_key={api_key}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "    # Return the whole movie dictionary, not just the title\n",
    "    return [movie['title'] for movie in data['cast']], [movie['id'] for movie in data['cast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(page, session):\n",
    "    url = f\"https://api.themoviedb.org/3/person/popular?api_key={api_key}&page={page}\"\n",
    "    response = session.get(url)\n",
    "    data = response.json()\n",
    "    people = []\n",
    "    for person in data['results']:\n",
    "        person_url = f\"https://api.themoviedb.org/3/person/{person['id']}?api_key={api_key}\"\n",
    "        person_response = session.get(person_url)\n",
    "        person_data = person_response.json()\n",
    "        people.append((person['name'], person['id'], person_data['gender'], person_data['birthday'], person_data['place_of_birth']))\n",
    "    return people\n",
    "\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        pages = list(range(1, 251))\n",
    "        fetch_page_with_session = partial(fetch_page, session=session)\n",
    "        people = list(executor.map(fetch_page_with_session, pages))\n",
    "\n",
    "all_people_names, all_people_ids, all_people_genders, all_people_birthdays, all_people_birthplaces = zip(*itertools.chain(*people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with 'actors' column\n",
    "df_actors = pd.DataFrame(all_people_names, columns=['actors'])\n",
    "\n",
    "# Add 'ids', 'genders', and 'birthplaces' columns to the DataFrame\n",
    "df_actors['actor_ids'] = all_people_ids\n",
    "df_actors['genders'] = all_people_genders\n",
    "df_actors['ages'] = all_people_birthdays # This is not the age, but the birthday\n",
    "df_actors['birthplaces'] = all_people_birthplaces\n",
    "\n",
    "# Change birthplaces so that it only contains the country (text after the last comma)\n",
    "df_actors['birthplaces'] = df_actors['birthplaces'].str.split(',').str[-1]\n",
    "\n",
    "# Change birthday to age\n",
    "df_actors['ages'] = pd.to_datetime(df_actors['ages'], errors='coerce')\n",
    "df_actors['ages'] = (pd.to_datetime('today') - df_actors['ages']).dt.days // 365\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_actors.dropna(inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df_actors.drop_duplicates(subset='actors', inplace=True)\n",
    "#reset index\n",
    "df_actors.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch movies for each actor and add them to 'movies' and 'movie_IDs' columns\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    movie_titles_and_ids_from_actor_ID_with_session = partial(movie_titles_and_IDs_from_actor_ID, session=session)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        movies_and_ids = list(executor.map(movie_titles_and_ids_from_actor_ID_with_session, df_actors['actor_ids']))\n",
    "\n",
    "# Add 'movies' column to df_actors\n",
    "df_actors['movies'] = [x[0] for x in movies_and_ids]\n",
    "\n",
    "# Flatten movies_and_ids into two separate lists\n",
    "movies = [item for sublist in [x[0] for x in movies_and_ids] for item in sublist]\n",
    "ids = [item for sublist in [x[1] for x in movies_and_ids] for item in sublist]\n",
    "\n",
    "# Convert the lists into a list of dictionaries\n",
    "movies_and_ids_dict = [{'movie': movie, 'movie_ID': id} for movie, id in zip(movies, ids)]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "movies_df = pd.DataFrame(movies_and_ids_dict)\n",
    "movies_df = movies_df.drop_duplicates()\n",
    "movies_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(session, url):\n",
    "    future = session.get(url, headers=headers)\n",
    "    return future\n",
    "\n",
    "def fetch_all(urls):\n",
    "    with FuturesSession() as session:\n",
    "        futures = [fetch(session, url) for url in urls]\n",
    "        responses = [future.result().json() for future in futures]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the URLs\n",
    "urls = [f\"https://api.themoviedb.org/3/movie/{id}?api_key={api_key}\" for id in movies_df[\"movie_ID\"]]\n",
    "print('urls are prepared')\n",
    "\n",
    "# Fetch all responses\n",
    "responses = fetch_all(urls)\n",
    "print('all responses are fetched')\n",
    "\n",
    "# Process the responses\n",
    "for i, response in enumerate(responses):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing response {i+1}/{len(responses)}\")\n",
    "    if isinstance(response, Exception):\n",
    "        print(f\"Error: {response}\")\n",
    "        continue  # Skip this response\n",
    "    # Process the response here\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "ratings = []\n",
    "popularities = []\n",
    "genres = []\n",
    "release_dates = []\n",
    "abstracts = []\n",
    "\n",
    "# Process the responses one by one\n",
    "for data in tqdm(responses):\n",
    "    ratings.append(data.get('vote_average'))\n",
    "    popularities.append(data.get('popularity'))\n",
    "    genres.append([genre['name'] for genre in data.get('genres', [])])\n",
    "    release_dates.append(data.get('release_date'))\n",
    "    abstracts.append(data.get('overview'))\n",
    "    \n",
    "# Assign the lists to the DataFrame columns\n",
    "movies_df['rating'] = ratings\n",
    "movies_df['popularity'] = popularities\n",
    "movies_df['genres'] = genres\n",
    "movies_df['release_date'] = release_dates\n",
    "movies_df['abstract'] = abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "movies_df.drop_duplicates(subset='movie', inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "movies_df.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with empty lists in 'genres' column\n",
    "movies_df = movies_df[movies_df['genres'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Remove rows with empty release dates\n",
    "movies_df = movies_df[movies_df['release_date'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if abstract is missing\n",
    "movies_df = movies_df[movies_df['abstract'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop row if rating is missing\n",
    "movies_df = movies_df[movies_df['rating'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Drop row if popularity is missing\n",
    "movies_df = movies_df[movies_df['popularity'].apply(lambda x: x > 0)]\n",
    "\n",
    "# Reset index\n",
    "movies_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes so that each actor is associated with the movies they have acted in\n",
    "df_actors_filtered = df_actors.copy()\n",
    "df_movies_filtered = movies_df.copy()\n",
    "df_movies_filtered.rename(columns={'movie': 'movies'}, inplace=True)\n",
    "\n",
    "df_actors_filtered = df_actors_filtered.explode('movies').reset_index(drop=True)\n",
    "\n",
    "df_actors_movies = df_actors_filtered.merge(df_movies_filtered, on='movies', how='left')\n",
    "\n",
    "# Remove all rows where the release date is before 2010 or after 2024 (to avoid movies that have not been released yet)\n",
    "df_actors_movies = df_actors_movies[(df_actors_movies['release_date'] >= '2010-01-01') & (df_actors_movies['release_date'] <= '2024-04-16')]\n",
    "\n",
    "# Collapse the the actors in the actor column so there is only one row per actor and the movies and movie_IDs are stored in lists\n",
    "df_actors_filtered = df_actors_movies.groupby('actors').agg({'actor_ids': 'first',\n",
    "                                                             'genders': 'first',\n",
    "                                                             'birthplaces': 'first',\n",
    "                                                             'ages': 'first',\n",
    "                                                             'movies': list, \n",
    "                                                             'movie_ID': list}).reset_index()\n",
    "\n",
    "df_movies_filtered = df_actors_movies.drop_duplicates(subset=['movies']).groupby('movies').agg({'movie_ID': 'first',\n",
    "                                                                                               'rating': 'first',\n",
    "                                                                                               'popularity': 'first',\n",
    "                                                                                               'genres': 'first',\n",
    "                                                                                               'release_date': 'first',\n",
    "                                                                                               'abstract': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the birthplace column, to avoid having the same country spelled differently etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import re\n",
    "\n",
    "# Clean birthplace column in actor dataframe\n",
    "len_bef_clean = len(df_actors_filtered['birthplaces'].unique())\n",
    "\n",
    "country_names = [country.name for country in pycountry.countries]\n",
    "\n",
    "def normalize_country_name(name):\n",
    "    # strip name\n",
    "    name = name.strip()\n",
    "    # replace '.' with ''\n",
    "    name = name.replace('.', '').replace(']', '')\n",
    "\n",
    "    if \"UK\" in name or \"İngiltere\" in name:\n",
    "        return \"United Kingdom\"\n",
    "    try:\n",
    "        # Try to get the country object\n",
    "        country = pycountry.countries.get(name=name)\n",
    "        if country is not None:\n",
    "            # If the country object is found, return the official name\n",
    "            return country.name\n",
    "        else:\n",
    "            # If the country object is not found, try to find it by its common name\n",
    "            country = pycountry.countries.search_fuzzy(name)\n",
    "            return country[0].name\n",
    "    except LookupError:\n",
    "        # Standardizing names\n",
    "        for country_name in country_names:\n",
    "            if country_name in name:\n",
    "                name = country_name\n",
    "\n",
    "        # Fixing abbreviations and wird instances\n",
    "        if \"Russia\" in name:\n",
    "            return \"Russian Federation\"\n",
    "        elif \"Türkiye\" in name or \"Turkey\" in name:\n",
    "            return \"Türkiye\"\n",
    "        elif \"USA\" in name or \" US\" in name or \"United States\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Korea\" in name:\n",
    "            return \"Korea, Republic of\"\n",
    "        elif \"Czech\" in name:\n",
    "            return \"Czechia\"\n",
    "        # Hardcoded, could use package to translate, maybe not necessary, few occurences\n",
    "        elif \"TX\" in name:\n",
    "            return \"United States\"\n",
    "        elif \"Frankrike\" in name:\n",
    "            return \"France\"\n",
    "        elif \"Afrique du Sud\" in name:\n",
    "            return \"South Africa\"\n",
    "        elif \"Irlanda\" in name:\n",
    "            return \"Ireland\"\n",
    "        elif \"中国\" in name or \"中华民国\" in name or \"重庆\" in name or \"南京\" in name:\n",
    "            return \"China\"\n",
    "        \n",
    "        \n",
    "        # Updating from old names\n",
    "        if \"now\" in name:\n",
    "            match = re.search(r'\\[now (.*?)', name)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        # If the country is not found, return the original name\n",
    "        return name\n",
    "\n",
    "# Normalize the country names in the DataFrame\n",
    "df_actors_filtered['birthplaces'] = df_actors_filtered['birthplaces'].apply(normalize_country_name)\n",
    "\n",
    "len_aft_clean = len(df_actors_filtered['birthplaces'].unique())\n",
    "print(\"length before cleaning\", len_bef_clean)\n",
    "print(\"length after cleaning\", len_aft_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all popularity scores above 100 to 100, to mitigate the effect of outliers\n",
    "df_movies_filtered['popularity'] = df_movies_filtered['popularity'].clip(upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram\n",
    "counts, bin_edges = np.histogram(df_movies_filtered['popularity'], bins=100)\n",
    "\n",
    "# Add a small constant to counts to avoid log(0)\n",
    "counts = counts + 1e-10\n",
    "\n",
    "# Plot the histogram\n",
    "plt.plot(bin_edges[:-1], counts)\n",
    "\n",
    "# Set the scale of both axes to logarithmic\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_movies_filtered['popularity'], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the age distribution, NOTHINGS DONE YET\n",
    "df_actors_filtered['ages'].hist(bins=30, edgecolor='white')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "# Get the counts of each unique value in the 'birthplace' column\n",
    "birthplace_counts = df_actors_filtered['birthplaces'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each birthplace out of the total number of actors\n",
    "birthplace_percentages = (birthplace_counts / df_actors_filtered['birthplaces'].count()) * 100\n",
    "\n",
    "# Get the top 5 birthplaces\n",
    "top_5_birthplaces = birthplace_percentages[:10]\n",
    "\n",
    "# Create a color map\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(top_5_birthplaces)))\n",
    "\n",
    "# Create a bar plot of the top 5 birthplaces with different colors for each bar\n",
    "top_5_birthplaces.plot(kind='bar', color=colors, edgecolor='black')\n",
    "\n",
    "plt.title('Top 10 Birthplaces of Actors')\n",
    "plt.xlabel('Birthplace')\n",
    "plt.ylabel('Percentage of Actors (%)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The number of unique birthplaces:\",len(df_actors_filtered['birthplaces'].unique()))\n",
    "# Get the birthplaces with a count of 1\n",
    "birthplaces_with_count_1 = birthplace_counts[birthplace_counts == 1]\n",
    "\n",
    "print(\"Number of countries only associated with One actor in our dataset\",len(birthplaces_with_count_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TMBD very biased towards USA with their popular actors data that we have retrieved (over 50% of the total number of actors). Therefore most of the movies are american and the actors from different countries are biased towards the american movie industry.\n",
    "\n",
    "It is also worth noting that 45 of the 109 unique birthplaces in our dataset is only associated with one actor, which is also a factor causing the unexpected low assortativity for the birthplace attribute. \n",
    "\n",
    "To fix this or make it more representative, we could webscrape from other websites to make a less biased dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv files\n",
    "df_actors_filtered.to_csv('data/actors.csv', index=False)\n",
    "df_movies_filtered.to_csv('data/movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes from csv files\n",
    "df_actors_filtered = pd.read_csv('data/actors.csv', converters={'movies': ast.literal_eval, 'movie_ID': ast.literal_eval})\n",
    "df_movies_filtered = pd.read_csv('data/movies.csv', converters={'genres': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Data\n",
    "movie_to_actors = defaultdict(list)\n",
    "\n",
    "for _ , row in df_actors_filtered.iterrows():\n",
    "    actor = row['actors']\n",
    "    movies = row['movies']\n",
    "    for movie in movies:\n",
    "        movie_to_actors[movie].append(actor)\n",
    "\n",
    "edges = defaultdict(int)\n",
    "for actors in movie_to_actors.values():\n",
    "    for pair in itertools.combinations(sorted(actors), 2):\n",
    "        edges[pair] += 1\n",
    "\n",
    "# Step 2: Create the Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "for _ , row in df_actors_filtered.iterrows():\n",
    "    G.add_node(row['actors'], gender=row['genders'], age=row['ages'], birthplace=row['birthplaces'])\n",
    "\n",
    "# Add edges with weights\n",
    "for edge, weight in edges.items():\n",
    "\n",
    "# YAHNI\n",
    "    if weight >= 2:\n",
    "        G.add_edge(*edge, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of nodes and edges\n",
    "N = G.number_of_nodes()\n",
    "L = G.number_of_edges()\n",
    "\n",
    "print(\"Number of nodes: \", N)\n",
    "print(\"Number of edges: \", L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, theory and analysis\n",
    "* Talk about how you’ve worked with text, including regular expressions, unicode, etc.\n",
    "\n",
    "* Describe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\n",
    "\n",
    "* How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Degrees and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute node degrees\n",
    "degrees = dict(G.degree())\n",
    "weighted_degrees = dict(G.degree(weight='weight'))\n",
    "\n",
    "# Compute the specificied values\n",
    "average_degree = np.mean(list(degrees.values()))\n",
    "median_degree = np.median(list(degrees.values()))\n",
    "try:\n",
    "    mode_degree = mode(list(degrees.values())) #Degree value that occurs with highest frequency among the nodes\n",
    "except:\n",
    "    mode_degree = \"No unique mode\"\n",
    "min_degree = min(degrees.values())\n",
    "max_degree = max(degrees.values())\n",
    "\n",
    "# Same calculations but for STRENGTH (WEIGHTED DEGREE)\n",
    "average_weighted_degree = np.mean(list(weighted_degrees.values()))\n",
    "median_weighted_degree = np.median(list(weighted_degrees.values()))\n",
    "try:\n",
    "    mode_weighted_degree = mode(list(weighted_degrees.values()))\n",
    "except:\n",
    "    mode_weighted_degree = \"No unique mode\"\n",
    "min_weighted_degree = min(weighted_degrees.values())\n",
    "max_weighted_degree = max(weighted_degrees.values())\n",
    "\n",
    "print(\"Degree Analysis:\")\n",
    "print(f\"Average Degree: {average_degree}\")\n",
    "print(f\"Median Degree: {median_degree}\")\n",
    "print(f\"Mode Degree: {mode_degree}\")\n",
    "print(f\"Minimum Degree: {min_degree}\")\n",
    "print(f\"Maximum Degree: {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree Analysis:\")\n",
    "print(f\"Average Weighted Degree: {average_weighted_degree}\")\n",
    "print(f\"Median Weighted Degree: {median_weighted_degree}\")\n",
    "print(f\"Mode Weighted Degree: {mode_weighted_degree}\")\n",
    "print(f\"Minimum Weighted Degree: {min_weighted_degree}\")\n",
    "print(f\"Maximum Weighted Degree: {max_weighted_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the edges by weight in descending order\n",
    "sorted_edges = sorted(G.edges.data(), key=lambda x: x[2]['weight'], reverse=True)\n",
    "most_connected_actors = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "most_connected_actors_weighted = sorted(weighted_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 5 actors with the highest degree:\")\n",
    "for i, actor in enumerate(most_connected_actors[:5], start=1):\n",
    "    print(f\"Rank: {i}, Actor: {actor[0]}, Degree: {actor[1]}\")\n",
    "\n",
    "print(\"\\nTop 5 actors with the highest weighted degree:\")\n",
    "for i, actor in enumerate(most_connected_actors_weighted[:5], start=1):\n",
    "    print(f\"Rank: {i}, Actor: {actor[0]}, Weighted Degree: {actor[1]}\")\n",
    "\n",
    "print(\"\\nTop 5 most important edges:\")\n",
    "for i, edge in enumerate(sorted_edges[:5], start=1):\n",
    "    print(f\"Rank: {i}, Edge: {edge[0]} - {edge[1]}, Weight: {edge[2]['weight']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top ten actors who have participated the most together, as the weight of an edge represents the number of times two actors (nodes) have co-acted in movies together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree distribution\n",
    "degree_sequence = sorted([d for n, d in G.degree()], reverse=False)  # sort in ascending order\n",
    "degree_count = nx.degree_histogram(G)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# degree distribution\n",
    "axs[0].bar(range(len(degree_count)), degree_count, align='center')\n",
    "axs[0].axvline(average_degree, color='r', linestyle='-', label='Average Degree')\n",
    "axs[0].set_xlabel('Degree')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_title('Degree Distribution')\n",
    "axs[0].legend()\n",
    "\n",
    "# loglog degree distribution\n",
    "axs[1].bar(range(len(degree_count)), degree_count, align='center')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('Degree')\n",
    "axs[1].set_ylabel('Count')\n",
    "axs[1].set_title('Degree Distribution (log scale)')\n",
    "axs[1].axvline(average_degree, color='r', linestyle='-', label='Average Degree')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probability that makes the expected number of edges equal to the actual number of edges in the graph.\n",
    "p = 2*L/(N*(N-1))\n",
    "print(\"Prob:\",p)\n",
    "\n",
    "# Compute the natural logarithm of N\n",
    "ln_N = np.log(N)\n",
    "\n",
    "print(f\"ln(N): {ln_N:.2f} < k: {average_degree:.2f}\\nAnd p: {p:.5f} > ln(N)/N: {ln_N/N:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the average degree is 10.22 and thus <k> > 1, and it is larger than ln(N) while p > ln(N)/N, the network must fall into the connected regime. It is therefore above the critical threshold. However, as we have thresholded the network by removing edges with an weight less than 2, it is not fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with netwulf\n",
    "import netwulf as nw\n",
    "\n",
    "#nw.visualize(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree assortativity coefficient\n",
    "degree_assortativity = nx.degree_assortativity_coefficient(G)\n",
    "\n",
    "# Calculate the attribute assortativity coefficient\n",
    "country_assortativity = nx.attribute_assortativity_coefficient(G, 'birthplace')\n",
    "\n",
    "gender_assortativity = nx.attribute_assortativity_coefficient(G, 'gender')\n",
    "\n",
    "# Calculate the numeric assortativity coefficient\n",
    "age_assortativity = nx.numeric_assortativity_coefficient(G, 'age')\n",
    "\n",
    "\n",
    "print(f\"degree_assortativity: {degree_assortativity}, country_assortativity: {country_assortativity}, gender_assortativity: {gender_assortativity}\")\n",
    "\n",
    "print(f\"age_assortativity: {age_assortativity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node to country assortativity: expected a higher value, but the litl lower value can be due to several reasons, desired nationality diversity, moving to different countries (living place) etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the isolates\n",
    "isolates = list(nx.isolates(G))\n",
    "\n",
    "# Print the number of isolates\n",
    "print(len(isolates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_movies_filtered['rating']\n",
    "# x2 = df_movies_filtered['popularity']\n",
    "# plt.scatter(x,x2) #ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the closeness centrality of the network\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Sort the actors according to the closeness centrality\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most central actors\n",
    "most_central_actors = sorted_closeness_centrality[:10]\n",
    "most_central_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the eigenvector centrality of the network\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Sort the scientists according to the eigenvector centrality\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most central actors\n",
    "most_central_actors2 = sorted_eigenvector_centrality[:10]\n",
    "most_central_actors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closeness centrality vs node degree to see if there is a correlation\n",
    "closeness_centrality_values = list(closeness_centrality.values())\n",
    "eigenvector_centrality_values = list(eigenvector_centrality.values())\n",
    "degree_values = list(dict(G.degree()).values())\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# plot the closeness centrality vs node degree to see if there is a correlation\n",
    "axs[0].scatter(degree_values, closeness_centrality_values, alpha=0.5)\n",
    "axs[0].set_xlabel('Node Degree')\n",
    "axs[0].set_ylabel('Closeness Centrality')\n",
    "axs[0].set_title('Closeness Centrality vs Node Degree')\n",
    "\n",
    "# plot the eigenvector centrality vs node degree to see if there is a correlation\n",
    "axs[1].scatter(degree_values, eigenvector_centrality_values, alpha=0.5)\n",
    "axs[1].set_xlabel('Node Degree')\n",
    "axs[1].set_ylabel('Eigenvector Centrality')\n",
    "axs[1].set_title('Eigenvector Centrality vs Node Degree')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Define a function to tokenize and clean text\n",
    "def tokenize_and_clean_text3(text, collocations): #takes text and dictionary of collocations\n",
    "    # Exclude URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Exclude mathematical symbols and numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Exclude punctuation and convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text).lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer = MWETokenizer(list(collocations.keys()))\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(tokens)\n",
    "    # stem\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('english')]# TO REMOVE STOPWORDS \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume abstracts is a list of movie abstracts\n",
    "abstracts = df_movies_filtered['abstract'].copy().tolist()\n",
    "\n",
    "# Tokenize the abstracts and remove stopwords\n",
    "tokens = [word for abstract in abstracts for word in word_tokenize(abstract.lower()) if word not in stopwords.words('english')] # maybe delete, also in function\n",
    "\n",
    "# Find bigram collocations\n",
    "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# Get the top 1000 bigrams by the Pointwise Mutual Information measure\n",
    "top_bigrams = bigram_finder.nbest(bigram_measures.pmi, 1000)\n",
    "\n",
    "# Convert the list of tuples into a dictionary\n",
    "collocations = defaultdict(int)\n",
    "for bigram in top_bigrams:\n",
    "    collocations[bigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_filtered['tokens'] = df_movies_filtered['abstract'].apply(lambda x: tokenize_and_clean_text3(x, collocations=collocations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "# Compute the best partition using the Louvain method\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Compute the modularity of this partition\n",
    "modularity = community_louvain.modularity(partition, G)\n",
    "\n",
    "print(\"Modularity found by Louvain algorithm:\", modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of communities and their sizes\n",
    "communities = {}\n",
    "for node, community in partition.items():\n",
    "    if community not in communities:\n",
    "        communities[community] = []\n",
    "    communities[community].append(node)\n",
    "\n",
    "num_communities = len(communities)\n",
    "community_sizes = sorted([len(nodes) for nodes in communities.values()], reverse=True)\n",
    "\n",
    "print(\"Number of communities:\", num_communities)\n",
    "print(\"Sizes of communities:\", community_sizes)\n",
    "# Check if the modularity is significantly different than 0\n",
    "if abs(modularity) > 0.01:  # or any other threshold you consider significant\n",
    "    print(\"The modularity is significantly different than 0.\")\n",
    "else:\n",
    "    print(\"The modularity is not significantly different than 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the partition and degree information\n",
    "communities_df = pd.DataFrame({\n",
    "    'actors': list(G.nodes),\n",
    "    'community': [partition[node] for node in G.nodes],\n",
    "    'degree': [G.degree(node) for node in G.nodes]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge actors_works_df with communities_df\n",
    "df = pd.merge(df_actors_filtered, communities_df, on='actors')\n",
    "df = df.drop(columns=[\"movies\"]).explode('movie_ID')\n",
    "\n",
    "\n",
    "# Merge with abstracts_df\n",
    "df = pd.merge(df, df_movies_filtered, left_on='movie_ID', right_on='movie_ID')\n",
    "\n",
    "# # Get abstract tokens for all communities\n",
    "all_communities_abstracts = df.groupby('community')['tokens'].agg(\"sum\").reset_index()\n",
    "\n",
    "all_communities_abstracts.columns = ['Community', 'Abstract Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# The top 5 communities\n",
    "top5_communities = communities_df['community'].value_counts().nlargest(5).index\n",
    "\n",
    "# Filter dataframe for top 5 communities\n",
    "top5_communities_abstracts = all_communities_abstracts[all_communities_abstracts['Community'].isin(top5_communities)]\n",
    "\n",
    "\n",
    "top_terms = {}\n",
    "for community in top5_communities:\n",
    "    # Get the abstract tokens for the community\n",
    "    abstract_tokens = top5_communities_abstracts[top5_communities_abstracts['Community'] == community]['Abstract Tokens'].values[0]\n",
    "    # Get the TF of each term\n",
    "    term_counts = Counter(abstract_tokens)\n",
    "    # top 5 most frequent terms\n",
    "    top_terms[community] = term_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_terms.items():\n",
    "    print(f\"Community {item[0]}:\")\n",
    "    print(f\"Top 5 terms: {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "# Get the top 9 communities and filter the abstracts for these communities\n",
    "top9_communities = communities_df['community'].value_counts().nlargest(9).index\n",
    "top9_communities_abstracts = all_communities_abstracts[all_communities_abstracts['Community'].isin(top9_communities)]\n",
    "\n",
    "top_terms = {}\n",
    "top_tfidf_terms = {}\n",
    "all_tfidf_terms = {}\n",
    "\n",
    "# (for less computational cost) get IDF for each term once and store the results\n",
    "idf_dict = {}\n",
    "for term in tqdm(set.union(*top9_communities_abstracts['Abstract Tokens'].apply(set))):\n",
    "    idf_dict[term] = math.log(len(top9_communities_abstracts) / \n",
    "                              sum(term in abstract for abstract in top9_communities_abstracts['Abstract Tokens']))\n",
    "\n",
    "\n",
    "for community in tqdm(top9_communities):\n",
    "    # Get abstract tokens for the community\n",
    "    abstract_tokens = top9_communities_abstracts[top9_communities_abstracts['Community'] == community]['Abstract Tokens'].values[0]\n",
    "    \n",
    "    # The top 10 TF terms\n",
    "    term_counts = Counter(abstract_tokens)\n",
    "    top_terms[community] = term_counts.most_common(10)\n",
    "\n",
    "\n",
    "    tfidf = {}\n",
    "    for term, count in term_counts.items():\n",
    "        # TF for all terms in top 9 communities\n",
    "        tf = count / len(abstract_tokens)\n",
    "\n",
    "        # Get IDF from the precalculated dictionary\n",
    "        idf = idf_dict[term]\n",
    "        \n",
    "        # TF-IDF for all terms in top 9 communities\n",
    "        tfidf[term] = tf * idf\n",
    "\n",
    "    all_tfidf_terms[community] = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top 10 TF-IDF words\n",
    "    top_tfidf_terms[community] = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in top_tfidf_terms.items():\n",
    "    print(f\"Community {item[0]}:\")\n",
    "    print(f\"Top 10 TF-IDF terms: {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top9_communities_df = communities_df[communities_df['community'].isin(top9_communities)]\n",
    "\n",
    "# Group by Community and Actor, and sum the Degree to get the total Degree for each Actor in each Community\n",
    "grouped_df = top9_communities_df.groupby(['community', 'actors'])['degree'].sum().reset_index()\n",
    "\n",
    "# Make new dataframe storing the top 3 actors by degree for each community\n",
    "top_actors = grouped_df.groupby('community').apply(lambda x: x.nlargest(5, 'degree')).reset_index(drop=True)\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files for wordcloud\n",
    "top_actors = pd.read_csv('data/top_actors.csv')\n",
    "\n",
    "top9_communities = pd.read_csv('data/top9_communities.csv')\n",
    "\n",
    "with open('data/all_tfidf_terms.json', 'r') as file:\n",
    "    all_tfidf_terms = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "fig, axs = plt.subplots(3, 3, figsize=(24, 24))\n",
    "\n",
    "for i, community in enumerate(top9_communities[\"community\"]):\n",
    "    # Get the already calculated top TF-IDF terms for the community\n",
    "    tfidf_terms = dict(all_tfidf_terms[str(community)])\n",
    "    \n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = None, \n",
    "                min_font_size = 10).generate_from_frequencies(tfidf_terms)\n",
    "    \n",
    "    # Subplot indices\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    # Plot the word cloud\n",
    "    axs[row, col].imshow(wordcloud)\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "    # Get the names of the top three actors in the community\n",
    "    top_three_actors = top_actors[top_actors['community'] == community]['actors'].values\n",
    "    # Create the title string with newlines\n",
    "    title_string = \"\\n\".join(top_three_actors)\n",
    "\n",
    "    # Set title of subplot to the names of the top three actors, each on a new line\n",
    "    axs[row, col].set_title(f\"Top actors in community {community}:\\n{title_string}\", fontsize=15)\n",
    "\n",
    "# Remove any extra subplots\n",
    "for j in range(i+1, 9):\n",
    "    fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "plt.tight_layout(pad = 1) \n",
    "plt.subplots_adjust(hspace = 0.25)  # Increase the hspace value\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Save all_tfidf_terms as a JSON file in the data folder\n",
    "with open('data/all_tfidf_terms.json', 'w') as f:\n",
    "    json.dump(all_tfidf_terms, f)\n",
    "\n",
    "# Save top9_communities as a CSV file in the data folder\n",
    "pd.DataFrame(top9_communities, columns=['community']).to_csv('data/top9_communities.csv', index=False)\n",
    "\n",
    "# Save top_actors as a CSV file in the data folder\n",
    "top_actors.to_csv('data/top_actors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "* What went well?\n",
    "* What is still missing? What could be improved? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
